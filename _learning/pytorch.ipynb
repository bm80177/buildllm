{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b50afee",
   "metadata": {},
   "source": [
    "# Learning Pytorch\n",
    "PyTorch is an open-source machine learning framework developed by Facebook's AI Research lab (FAIR). It's primarily used for deep learning applications, such as computer vision and natural language processing.\n",
    "\n",
    "Key characteristics of PyTorch include:\n",
    "\n",
    "**Pythonic**: It's designed to be deeply integrated with the Python language, making it feel more natural and easier to debug using standard Python tools.\n",
    "\n",
    "**Tensors**: Its fundamental data structure is a Tensor, which is a multi-dimensional array similar to NumPy arrays, but with the added capability to utilize GPUs for accelerated computation.\n",
    "\n",
    "**Imperative and Dynamic**: PyTorch is imperative. It excutes the code immediately, step-by-step like standard python program. While executing the code, it builds the computational graph on the fly dynamically. The computational graph (the map of math operations) is constructed on the fly dynamically (meaning during execution, or \"define-by-run\"). It changes and adapts with every forward pass of data.\n",
    "\n",
    "The other poular framework is **TensorFlow** which was was originally developed by Google. It is strongly focused on production and scalability. Traditionally had a steeper learning curve, but the high-level **Keras API** makes model building much simpler now.\n",
    "\n",
    "## 5 Steps to Deep Learning with Pytorch\n",
    "- Prediction\n",
    "- Loss Calculation\n",
    "- Gradient Calculation\n",
    "- Param Update\n",
    "- Gradient Reset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b8111",
   "metadata": {},
   "source": [
    "# Creating Tensor\n",
    "Tensor is the building block of Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aaa19d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886]])\n",
      "\n",
      "\n",
      "tensor([[0.0740, 0.8665, 0.1366],\n",
      "        [0.1025, 0.1841, 0.7264]])\n",
      "\n",
      "\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# create tensor from data\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "print(\"\\n\")\n",
    "\n",
    "# create tensor from numpy array\n",
    "import numpy as np\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)\n",
    "print(\"\\n\")\n",
    "\n",
    "# create tensor from another tensor\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(x_ones)\n",
    "print(\"\\n\")\n",
    "\n",
    "torch.manual_seed(123) # Set seed for reproducibility when creating random tensors\n",
    "\n",
    "# Creating from a desired shape\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x\n",
    "print(x_rand)\n",
    "print(\"\\n\")\n",
    "# Specify shape\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "print(rand_tensor)\n",
    "print(\"\\n\")\n",
    "ones_tensor = torch.ones(shape)\n",
    "print(ones_tensor)\n",
    "print(\"\\n\")\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(zeros_tensor)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b1f03",
   "metadata": {},
   "source": [
    "# Critical Attributes for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.rand(3,4) # float is the default datatype\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\") \n",
    "print(f\"Device tensor is stored on: {tensor.device}\") # tells where tensor lives, cpu or cuda. cuda is for nvidia gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f9bad",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "Autograd (short for Automatic Differentiation) is the engine that modern deep learning frameworks like PyTorch and TensorFlow use to automatically calculate the gradients needed to train a neural network.\n",
    "\n",
    "requires_grad=True tells Autograd to build computational graph for the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Tensor:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Requires Grad: False\n",
      "\n",
      "\n",
      "Parameter Tensor:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# A standard data tensor\n",
    "x_data = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# A parameter tensor that requires gradients\n",
    "x_param = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True) # pytorch builds a computational graph for this tensor\n",
    "\n",
    "print(\"Data Tensor:\\n\", x_data)\n",
    "print(\"Requires Grad:\", x_data.requires_grad)\n",
    "print(\"\\n\")\n",
    "print(\"Parameter Tensor:\\n\", x_param)\n",
    "print(\"Requires Grad:\", x_param.requires_grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52a990a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn for d: <AddBackward0 object at 0x77076bf61d20>\n",
      "grad_fn for c: <AddBackward0 object at 0x77076b8bb2e0>\n",
      "grad_fn for b: None\n",
      "grad_fn for a: None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "d = a*b + b**2 + c*3\n",
    "\n",
    "print(f\"grad_fn for d: {d.grad_fn}\")  # grad_fn shows the function that generated this tensor\n",
    "print(f\"grad_fn for c: {c.grad_fn}\")  # grad_fn shows the function that generated this tensor\n",
    "print(f\"grad_fn for b: {b.grad_fn}\")  # None because a is created by user\n",
    "print(f\"grad_fn for a: {a.grad_fn}\")  # None because a is created by user\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f611e",
   "metadata": {},
   "source": [
    "# Pytorch Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c8197",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "* is used for element-wise multiplication\n",
    "@ is used for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise Multiplication:\n",
      " tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n",
      "\n",
      "Matrix Multiplication:\n",
      " tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1.0, 2.0], \n",
    "                  [3.0, 4.0],])\n",
    "\n",
    "b = torch.tensor([[5.0, 6.0], \n",
    "                  [7.0, 8.0]])\n",
    "\n",
    "elementwise_multiplication = a * b\n",
    "matrix_multiplication = a @ b\n",
    "\n",
    "# Note: element-wise multiplication expects both tensors to be of same shape\n",
    "print(\"Element-wise Multiplication:\\n\", elementwise_multiplication) \n",
    "print(\"\\n\")\n",
    "# Note: matrix multiplication expects the inner dimensions to match. \n",
    "# i.e. the number of columns in the first matrix should be equal to the number of rows in the second matrix.\n",
    "print(\"Matrix Multiplication:\\n\", matrix_multiplication)\n",
    "\n",
    "# Note: In linear algebra (y = m.x + c), matrix multiplication is often denoted using a dot (m¬∑x) or simply by juxtaposition (mx).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edac9d",
   "metadata": {},
   "source": [
    "## Reduction Operation and dim Argument\n",
    "Reduction operations are mean(), min(), max(), sum()...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "798d94a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score of all elements: tensor(18.8889)\n",
      "Average Score per Column: tensor([18.3333, 20.0000, 18.3333])\n",
      "Average Score per Row: tensor([20.0000,  6.6667, 30.0000])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.tensor([[10.0, 20.0, 30.0],\n",
    "                       [5.0, 10.0, 5.0],\n",
    "                       [40.0, 30.0, 20.0]])\n",
    "\n",
    "average_score = scores.mean()  # mean of all elements\n",
    "print(\"Average Score of all elements:\", average_score)                 \n",
    "\n",
    "average_score = scores.mean(dim=0)  # mean across rows for each column\n",
    "print(\"Average Score per Column:\", average_score)\n",
    "\n",
    "average_score = scores.mean(dim=1)  # mean across columns for each row\n",
    "print(\"Average Score per Row:\", average_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa948c",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f86be50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78542736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd Column: tensor([ 2,  6, 10])\n",
      "2nd Row: tensor([4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "# Get the 3rd column\n",
    "third_column = x[:, 2]\n",
    "print(\"3rd Column:\", third_column)\n",
    "\n",
    "# Get the 2nd row\n",
    "second_row = x[1, :]\n",
    "print(\"2nd Row:\", second_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c43abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of Best Elements in Each Row: tensor([2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "scores = torch.tensor([[10.0, 20.0, 30.0],\n",
    "                       [5.0, 10.0, 5.0],\n",
    "                       [40.0, 30.0, 20.0]])\n",
    "\n",
    "best_indices = torch.argmax(scores,dim=1)  # Get indices of max elements in each row\n",
    "print(\"Indices of Best Elements in Each Row:\", best_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a862fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Rows:\n",
      " tensor([[[10., 20., 30.]],\n",
      "\n",
      "        [[40., 30., 20.]],\n",
      "\n",
      "        [[ 5., 10.,  5.]]])\n",
      "Selected Rows using gather:\n",
      " tensor([[10.],\n",
      "        [ 5.],\n",
      "        [30.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor([[10.0, 20.0, 30.0, 13],\n",
    "                       [5.0, 10.0, 5.0, 23],\n",
    "                       [40.0, 30.0, 20.0, 33]])\n",
    "\n",
    "# shopping list\n",
    "indices_to_select = torch.tensor([[0], [2], [1]])  # selecting 1st and 3rd rows\n",
    "\n",
    "selected_rows = data[indices_to_select]\n",
    "print(\"Selected Rows:\\n\", selected_rows)\n",
    "\n",
    "selected_rows = torch.gather(data, dim=1, index=indices_to_select)\n",
    "print(\"Selected Rows using gather:\\n\", selected_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec219cf",
   "metadata": {},
   "source": [
    "# Building a Model from Scratch\n",
    "\n",
    "$\\hat{y}$ = mX + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e77937",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "Forward Pass is the model's first guess. When you first creates the model, it knows nothing. It's guess is completely random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d9ed087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data X:\n",
      " tensor([[ 1.0139],\n",
      "        [ 0.8988],\n",
      "        [-0.2111],\n",
      "        [-1.5326],\n",
      "        [-0.6163],\n",
      "        [ 0.2288],\n",
      "        [-0.1120],\n",
      "        [-2.0506],\n",
      "        [-0.6189],\n",
      "        [-0.7804]])\n",
      "Output Data y_true:\n",
      " tensor([[ 4.9357],\n",
      "        [ 4.8457],\n",
      "        [ 2.5002],\n",
      "        [ 0.0214],\n",
      "        [ 1.8423],\n",
      "        [ 3.3763],\n",
      "        [ 2.8427],\n",
      "        [-1.1818],\n",
      "        [ 1.8532],\n",
      "        [ 1.5877]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Out batch of data will have 10 data points\n",
    "N = 10\n",
    "\n",
    "# Each data point has one input feature (x) and one output feature (y)\n",
    "D_in = 1\n",
    "D_out = 1\n",
    "\n",
    "# Create our input data X\n",
    "torch.manual_seed(24)  # For reproducibility\n",
    "X = torch.randn(N, D_in)\n",
    "print(\"Input Data X:\\n\", X)\n",
    "\n",
    "# Let's assume the true relationship is y = 2x + 3\n",
    "true_m = torch.tensor([[2.0]]) # slope in the linear requation y = mx + c\n",
    "true_c = torch.tensor(3) # intercept in the linear equation y = mx + c\n",
    "\n",
    "# Create output data y with some noise (0.1 * torch.randn(N, D_out))\n",
    "y = X @ true_m + true_c \n",
    "y_true = y + 0.1 * torch.randn(N, D_out) # Adding noise\n",
    "print(\"Output Data y_true:\\n\", y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1da729bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight: tensor([[1.5769]], requires_grad=True)\n",
      "Initial bias: tensor([0.1243], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# To build a model from scratch, we need to define parameters m and c\n",
    "# Initialize m and c randomly\n",
    "m = torch.randn(D_in, D_out, requires_grad=True)  # slope\n",
    "c = torch.randn(1, requires_grad=True)            # intercept\n",
    "print(\"Initial weight:\", m)\n",
    "print(\"Initial bias:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e578322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "train_size = int(0.8 * N)\n",
    "X_train = X[:train_size]\n",
    "y_train = y_true[:train_size]\n",
    "X_test = X[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "97f7aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "def model(X):\n",
    "    return X @ m + c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20660657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output y_pred:\n",
      " tensor([[ 1.7231],\n",
      "        [ 1.5416],\n",
      "        [-0.2086],\n",
      "        [-2.2925],\n",
      "        [-0.8476],\n",
      "        [ 0.4851],\n",
      "        [-0.0523],\n",
      "        [-3.1093]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# predict the output for training data\n",
    "y_pred = model(X_train)\n",
    "print(\"Predicted Output y_pred:\\n\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23ea56",
   "metadata": {},
   "source": [
    "## Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "03ee3e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Loss: tensor(7.7024, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "MSE_loss = torch.mean((y_pred - y_train) ** 2)\n",
    "print(\"Mean Squared Error Loss:\", MSE_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c0611",
   "metadata": {},
   "source": [
    "# üìò Calculating the Gradients\n",
    "\n",
    "The **gradient** tells us how much the loss $ L $ changes when we slightly change a parameter ($ w $ or $ b $).  \n",
    "We find this using **partial differentiation**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Gradient with Respect to the Weight ($ w $)\n",
    "\n",
    "We want to find $ \\frac{\\partial L}{\\partial w} $.  \n",
    "Using the **chain rule**:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w}\n",
    "$$ \n",
    "\n",
    "### Step 1: Find $ \\frac{\\partial L}{\\partial \\hat{y}} $\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = \n",
    "\\frac{\\partial}{\\partial \\hat{y}} \n",
    "\\left[ \\frac{1}{2} (y - \\hat{y})^2 \\right]\n",
    "= 2 \\cdot \\frac{1}{2} (y - \\hat{y}) \\cdot (-1)\n",
    "= - (y - \\hat{y})\n",
    "= \\hat{y} - y\n",
    "$$ \n",
    "\n",
    "This represents the **prediction error**.\n",
    "\n",
    "### Step 2: Find $ \\frac{\\partial \\hat{y}}{\\partial w} $\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\hat{y}}{\\partial w} = \n",
    "\\frac{\\partial}{\\partial w} [w \\cdot x + b] = x\n",
    "$$ \n",
    "\n",
    "### Step 3: Combine (Chain Rule)\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial w} = (\\hat{y} - y) \\cdot x\n",
    "$$ \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Gradient with Respect to the Bias ($ b $)\n",
    "\n",
    "We want to find $ \\frac{\\partial L}{\\partial b} $.  \n",
    "Again, we use the **chain rule**:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial b}\n",
    "$$ \n",
    "\n",
    "### Step 1: Find $ \\frac{\\partial L}{\\partial \\hat{y}} $\n",
    "\n",
    "(Same as before)\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\n",
    "$$ \n",
    "\n",
    "### Step 2: Find $ \\frac{\\partial \\hat{y}}{\\partial b} $\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\hat{y}}{\\partial b} = \n",
    "\\frac{\\partial}{\\partial b} [w \\cdot x + b] = 1\n",
    "$$ \n",
    "\n",
    "### Step 3: Combine (Chain Rule)\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial b} = (\\hat{y} - y) \\cdot 1 = \\hat{y} - y\n",
    "$$ \n",
    "\n",
    "---\n",
    "\n",
    "## üìù Summary of Gradients\n",
    "\n",
    "The calculated gradients are:\n",
    "\n",
    "- **Gradient of Loss w.r.t. Weight:**\n",
    "\n",
    "  $$ \n",
    "  \\frac{\\partial L}{\\partial w} = (\\hat{y} - y) \\cdot x\n",
    "  $$ \n",
    "\n",
    "- **Gradient of Loss w.r.t. Bias:**\n",
    "\n",
    "  $$ \n",
    "  \\frac{\\partial L}{\\partial b} = \\hat{y} - y\n",
    "  $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of m: tensor([[0.7908]])\n",
      "Gradient of c: tensor([-5.4857])\n"
     ]
    }
   ],
   "source": [
    "# backward pass to compute gradients\n",
    "# this single command computes the gradient of MSE_loss with respect to all tensors with requires_grad=True\n",
    "# It populates the .grad attribute of those tensors m and c\n",
    "MSE_loss.backward()\n",
    "\n",
    "# the gradients are stored in .grad attributes of m and c\n",
    "# the .grad attribute holds the gradient values that tells us how to change m and c to reduce the loss\n",
    "print(\"Gradient of m:\", m.grad)\n",
    "print(\"Gradient of c:\", c.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
