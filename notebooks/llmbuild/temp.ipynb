{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V5E1","authorship_tag":"ABX9TyP1n3Ohi8uwdNcLpS7/ctK7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["#### ðŸ“Š GPT-2 Small (117M) â€” Parameter Count Breakdown\n","\n","##### Model Specs\n","- **12 decoder blocks**\n","- **12 attention heads per block**\n","- **Hidden size:** 768  \n","- **FFN size:** 3072  \n","- **Vocabulary:** 50,257  \n","- **Context length:** 1024  \n","\n","---\n","\n","#### ðŸ§± 1. Embedding Layer\n","\n","##### Token Embedding\n","```\n","50,257 Ã— 768 = 38,610,816 params\n","```\n","\n","##### Positional Embedding\n","```\n","1024 Ã— 768 = 786,432 params\n","```\n","\n","###### âœ”ï¸ Total Embeddings = ~39.4M\n","\n","---\n","\n","#### ðŸ” 2. Decoder Block (Ã—12)\n","\n","Each of the 12 blocks contains:\n","\n","---\n","\n","##### ðŸ§² A. Masked Multi-Head Attention\n","\n","###### Q, K, V projection matrices\n","```\n","3 Ã— (768 Ã— 768) = 1,769,472\n","biases â‰ˆ 2,304\n","```\n","\n","###### Output projection\n","```\n","768 Ã— 768 = 589,824\n","bias â‰ˆ 768\n","```\n","\n","###### âœ”ï¸ Attention per block â‰ˆ 2.36M\n","\n","---\n","\n","##### âš™ï¸ B. Feed-Forward Network (FFN)\n","\n","###### Linear 1 (768 â†’ 3072)\n","```\n","768 Ã— 3072 = 2,359,296\n","bias = 3072\n","```\n","\n","###### Linear 2 (3072 â†’ 768)\n","```\n","3072 Ã— 768 = 2,359,296\n","bias = 768\n","```\n","\n","###### âœ”ï¸ FFN per block â‰ˆ 4.72M\n","\n","---\n","\n","##### ðŸ“ C. LayerNorms\n","Each block has 2 LayerNorms:\n","```\n","2 Ã— (768 + 768) = 3072 params\n","```\n","\n","###### âœ”ï¸ LayerNorm per block â‰ˆ 0.003M\n","\n","---\n","\n","##### ðŸ“¦ Total per Decoder Block\n","```\n","Attention:   ~2.36M\n","FFN:         ~4.72M\n","LayerNorms:   0.003M\n","--------------------------------\n","Total/block â‰ˆ 7.083M\n","```\n","\n","---\n","\n","#### ðŸ§® 3. Total for 12 Blocks\n","```\n","12 Ã— 7.083M = 84.996M â‰ˆ 85M\n","```\n","\n","---\n","\n","#### ðŸ”š 4. Output Layer (LM Head)\n","GPT-2 uses **weight tying** â†’ LM head shares weights with token embeddings.  \n","âœ”ï¸ No extra parameters\n","\n","---\n","\n","#### ðŸŽ¯ Final Parameter Count\n","```\n","Embeddings:   ~39.4M\n","12 Blocks:    ~85.0M\n","-------------------------\n","Total:       ~124M\n","```\n","\n","###### âš ï¸ Official number: **117M**  \n","Differences come from:\n","- tied embeddings  \n","- compression/optimization  \n","- rounding in documentation  \n","\n","---\n","\n","#### ðŸ“˜ Summary Table\n","\n","| Component | Params |\n","|----------|--------|\n","| Token Embeddings | 38.6M |\n","| Positional Embeddings | 0.79M |\n","| Multi-Head Attention | 28.3M |\n","| Feed-Forward | 56.6M |\n","| LayerNorm | 0.04M |\n","| **Total (Actual)** | **~124M** |\n","| **GPT-2 Small (Official)** | **117M** |\n","\n","\n"],"metadata":{"id":"7P_mpVEmRt4o"}},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pjSFCwCdL4bl","executionInfo":{"status":"ok","timestamp":1764715436719,"user_tz":360,"elapsed":1337,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"cc58cde2-e869-4021-d229-40dac8a8bf30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMhZTbu8JPRD","executionInfo":{"status":"ok","timestamp":1764723019777,"user_tz":360,"elapsed":5522671,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"a97af12e-78c9-49e9-a6f4-7d0a606c4214"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Batch 0, Loss: 10.9896\n","Epoch 1 finished, Avg Loss: 9.2457\n","Epoch 2, Batch 0, Loss: 7.8389\n","Epoch 2 finished, Avg Loss: 6.9950\n","Epoch 3, Batch 0, Loss: 6.1085\n","Epoch 3 finished, Avg Loss: 5.6197\n","Epoch 4, Batch 0, Loss: 4.9615\n","Epoch 4 finished, Avg Loss: 4.6463\n","Epoch 5, Batch 0, Loss: 4.1953\n","Epoch 5 finished, Avg Loss: 3.9221\n","Epoch 6, Batch 0, Loss: 3.5472\n","Epoch 6 finished, Avg Loss: 3.3249\n","Epoch 7, Batch 0, Loss: 2.9555\n","Epoch 7 finished, Avg Loss: 2.7789\n","Epoch 8, Batch 0, Loss: 2.5989\n","Epoch 8 finished, Avg Loss: 2.3812\n","Epoch 9, Batch 0, Loss: 2.0161\n","Epoch 9 finished, Avg Loss: 2.0621\n","Epoch 10, Batch 0, Loss: 1.7500\n","Epoch 10 finished, Avg Loss: 1.7780\n","Epoch 11, Batch 0, Loss: 1.5158\n","Epoch 11 finished, Avg Loss: 1.5352\n","Epoch 12, Batch 0, Loss: 1.4968\n","Epoch 12 finished, Avg Loss: 1.3012\n","Epoch 13, Batch 0, Loss: 1.2358\n","Epoch 13 finished, Avg Loss: 1.1061\n","Epoch 14, Batch 0, Loss: 0.9104\n","Epoch 14 finished, Avg Loss: 0.9164\n","Epoch 15, Batch 0, Loss: 0.8266\n","Epoch 15 finished, Avg Loss: 0.7340\n","Epoch 16, Batch 0, Loss: 0.5489\n","Epoch 16 finished, Avg Loss: 0.6154\n","Epoch 17, Batch 0, Loss: 0.4677\n","Epoch 17 finished, Avg Loss: 0.4654\n","Epoch 18, Batch 0, Loss: 0.3753\n","Epoch 18 finished, Avg Loss: 0.3637\n","Epoch 19, Batch 0, Loss: 0.2935\n","Epoch 19 finished, Avg Loss: 0.2918\n","Epoch 20, Batch 0, Loss: 0.2440\n","Epoch 20 finished, Avg Loss: 0.2293\n","Epoch 21, Batch 0, Loss: 0.2180\n","Epoch 21 finished, Avg Loss: 0.1921\n","Epoch 22, Batch 0, Loss: 0.1574\n","Epoch 22 finished, Avg Loss: 0.1665\n","Epoch 23, Batch 0, Loss: 0.1475\n","Epoch 23 finished, Avg Loss: 0.1461\n","Epoch 24, Batch 0, Loss: 0.1208\n","Epoch 24 finished, Avg Loss: 0.1246\n","Epoch 25, Batch 0, Loss: 0.1071\n","Epoch 25 finished, Avg Loss: 0.1154\n","Epoch 26, Batch 0, Loss: 0.1192\n","Epoch 26 finished, Avg Loss: 0.1096\n","Epoch 27, Batch 0, Loss: 0.0948\n","Epoch 27 finished, Avg Loss: 0.1068\n","Epoch 28, Batch 0, Loss: 0.1044\n","Epoch 28 finished, Avg Loss: 0.0925\n","Epoch 29, Batch 0, Loss: 0.0924\n","Epoch 29 finished, Avg Loss: 0.0943\n","Epoch 30, Batch 0, Loss: 0.0975\n","Epoch 30 finished, Avg Loss: 0.0964\n","Epoch 31, Batch 0, Loss: 0.1215\n","Epoch 31 finished, Avg Loss: 0.1030\n","Epoch 32, Batch 0, Loss: 0.0848\n","Epoch 32 finished, Avg Loss: 0.0854\n","Epoch 33, Batch 0, Loss: 0.0793\n","Epoch 33 finished, Avg Loss: 0.0803\n","Epoch 34, Batch 0, Loss: 0.0828\n","Epoch 34 finished, Avg Loss: 0.0809\n","Epoch 35, Batch 0, Loss: 0.0701\n","Epoch 35 finished, Avg Loss: 0.0736\n","Epoch 36, Batch 0, Loss: 0.0526\n","Epoch 36 finished, Avg Loss: 0.0594\n","Epoch 37, Batch 0, Loss: 0.0522\n","Epoch 37 finished, Avg Loss: 0.0539\n","Epoch 38, Batch 0, Loss: 0.0473\n","Epoch 38 finished, Avg Loss: 0.0523\n","Epoch 39, Batch 0, Loss: 0.0420\n","Epoch 39 finished, Avg Loss: 0.0475\n","Epoch 40, Batch 0, Loss: 0.0355\n","Epoch 40 finished, Avg Loss: 0.0433\n","Epoch 41, Batch 0, Loss: 0.0381\n","Epoch 41 finished, Avg Loss: 0.0424\n","Epoch 42, Batch 0, Loss: 0.0345\n","Epoch 42 finished, Avg Loss: 0.0370\n","Epoch 43, Batch 0, Loss: 0.0287\n","Epoch 43 finished, Avg Loss: 0.0329\n","Epoch 44, Batch 0, Loss: 0.0315\n","Epoch 44 finished, Avg Loss: 0.0317\n","Epoch 45, Batch 0, Loss: 0.0277\n","Epoch 45 finished, Avg Loss: 0.0310\n","Epoch 46, Batch 0, Loss: 0.0291\n","Epoch 46 finished, Avg Loss: 0.0336\n","Epoch 47, Batch 0, Loss: 0.0282\n","Epoch 47 finished, Avg Loss: 0.0353\n","Epoch 48, Batch 0, Loss: 0.0323\n","Epoch 48 finished, Avg Loss: 0.0407\n","Epoch 49, Batch 0, Loss: 0.0419\n","Epoch 49 finished, Avg Loss: 0.0439\n","Epoch 50, Batch 0, Loss: 0.0741\n","Epoch 50 finished, Avg Loss: 0.0623\n","Epoch 51, Batch 0, Loss: 0.0519\n","Epoch 51 finished, Avg Loss: 0.0542\n","Epoch 52, Batch 0, Loss: 0.0447\n","Epoch 52 finished, Avg Loss: 0.0504\n","Epoch 53, Batch 0, Loss: 0.0441\n","Epoch 53 finished, Avg Loss: 0.0606\n","Epoch 54, Batch 0, Loss: 0.0387\n","Epoch 54 finished, Avg Loss: 0.0508\n","Epoch 55, Batch 0, Loss: 0.0519\n","Epoch 55 finished, Avg Loss: 0.0646\n","Epoch 56, Batch 0, Loss: 0.0508\n","Epoch 56 finished, Avg Loss: 0.0519\n","Epoch 57, Batch 0, Loss: 0.0530\n","Epoch 57 finished, Avg Loss: 0.0457\n","Epoch 58, Batch 0, Loss: 0.0390\n","Epoch 58 finished, Avg Loss: 0.0383\n","Epoch 59, Batch 0, Loss: 0.0310\n","Epoch 59 finished, Avg Loss: 0.0298\n","Epoch 60, Batch 0, Loss: 0.0245\n","Epoch 60 finished, Avg Loss: 0.0291\n","Epoch 61, Batch 0, Loss: 0.0227\n","Epoch 61 finished, Avg Loss: 0.0257\n","Epoch 62, Batch 0, Loss: 0.0198\n","Epoch 62 finished, Avg Loss: 0.0229\n","Epoch 63, Batch 0, Loss: 0.0152\n","Epoch 63 finished, Avg Loss: 0.0195\n","Epoch 64, Batch 0, Loss: 0.0155\n","Epoch 64 finished, Avg Loss: 0.0177\n","Epoch 65, Batch 0, Loss: 0.0123\n","Epoch 65 finished, Avg Loss: 0.0136\n","Epoch 66, Batch 0, Loss: 0.0132\n","Epoch 66 finished, Avg Loss: 0.0128\n","Epoch 67, Batch 0, Loss: 0.0121\n","Epoch 67 finished, Avg Loss: 0.0118\n","Epoch 68, Batch 0, Loss: 0.0090\n","Epoch 68 finished, Avg Loss: 0.0095\n","Epoch 69, Batch 0, Loss: 0.0097\n","Epoch 69 finished, Avg Loss: 0.0083\n","Epoch 70, Batch 0, Loss: 0.0073\n","Epoch 70 finished, Avg Loss: 0.0073\n","Epoch 71, Batch 0, Loss: 0.0073\n","Epoch 71 finished, Avg Loss: 0.0073\n","Epoch 72, Batch 0, Loss: 0.0076\n","Epoch 72 finished, Avg Loss: 0.0072\n","Epoch 73, Batch 0, Loss: 0.0064\n","Epoch 73 finished, Avg Loss: 0.0067\n","Epoch 74, Batch 0, Loss: 0.0049\n","Epoch 74 finished, Avg Loss: 0.0067\n","Epoch 75, Batch 0, Loss: 0.0049\n","Epoch 75 finished, Avg Loss: 0.0065\n","Epoch 76, Batch 0, Loss: 0.0066\n","Epoch 76 finished, Avg Loss: 0.0063\n","Epoch 77, Batch 0, Loss: 0.0040\n","Epoch 77 finished, Avg Loss: 0.0055\n","Epoch 78, Batch 0, Loss: 0.0046\n","Epoch 78 finished, Avg Loss: 0.0056\n","Epoch 79, Batch 0, Loss: 0.0045\n","Epoch 79 finished, Avg Loss: 0.0045\n","Epoch 80, Batch 0, Loss: 0.0035\n","Epoch 80 finished, Avg Loss: 0.0045\n","Epoch 81, Batch 0, Loss: 0.0054\n","Epoch 81 finished, Avg Loss: 0.0046\n","Epoch 82, Batch 0, Loss: 0.0030\n","Epoch 82 finished, Avg Loss: 0.0039\n","Epoch 83, Batch 0, Loss: 0.0028\n","Epoch 83 finished, Avg Loss: 0.0031\n","Epoch 84, Batch 0, Loss: 0.0038\n","Epoch 84 finished, Avg Loss: 0.0030\n","Epoch 85, Batch 0, Loss: 0.0025\n","Epoch 85 finished, Avg Loss: 0.0029\n","Epoch 86, Batch 0, Loss: 0.0024\n","Epoch 86 finished, Avg Loss: 0.0027\n","Epoch 87, Batch 0, Loss: 0.0020\n","Epoch 87 finished, Avg Loss: 0.0024\n","Epoch 88, Batch 0, Loss: 0.0020\n","Epoch 88 finished, Avg Loss: 0.0022\n","Epoch 89, Batch 0, Loss: 0.0019\n","Epoch 89 finished, Avg Loss: 0.0020\n","Epoch 90, Batch 0, Loss: 0.0018\n","Epoch 90 finished, Avg Loss: 0.0019\n","Epoch 91, Batch 0, Loss: 0.0016\n","Epoch 91 finished, Avg Loss: 0.0018\n","Epoch 92, Batch 0, Loss: 0.0017\n","Epoch 92 finished, Avg Loss: 0.0017\n","Epoch 93, Batch 0, Loss: 0.0016\n","Epoch 93 finished, Avg Loss: 0.0016\n","Epoch 94, Batch 0, Loss: 0.0015\n","Epoch 94 finished, Avg Loss: 0.0016\n","Epoch 95, Batch 0, Loss: 0.0015\n","Epoch 95 finished, Avg Loss: 0.0015\n","Epoch 96, Batch 0, Loss: 0.0014\n","Epoch 96 finished, Avg Loss: 0.0015\n","Epoch 97, Batch 0, Loss: 0.0016\n","Epoch 97 finished, Avg Loss: 0.0014\n","Epoch 98, Batch 0, Loss: 0.0013\n","Epoch 98 finished, Avg Loss: 0.0014\n","Epoch 99, Batch 0, Loss: 0.0014\n","Epoch 99 finished, Avg Loss: 0.0013\n","Epoch 100, Batch 0, Loss: 0.0013\n","Epoch 100 finished, Avg Loss: 0.0014\n","Model saved as simple_gpt_model.pth\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import tiktoken\n","\n","# -----------------------------\n","# 1. PARAMETERS\n","# -----------------------------\n","batch_size = 8              # keep small for GPU memory, increase later\n","max_length = 1024\n","stride = 100\n","embedding_dim = 768\n","num_heads = 12\n","mlp_hidden_dim = 3072\n","num_layers = 12              # GPT-2 small = 12 layers, reduce for demo\n","lr = 3e-4\n","epochs = 100\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# -----------------------------\n","# 2. READ RAW TEXT\n","# -----------------------------\n","with open(\"./the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()\n","\n","# -----------------------------\n","# 3. TOKENIZER\n","# -----------------------------\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","tokens = tokenizer.encode(raw_text)\n","vocab_size = tokenizer.n_vocab\n","\n","# -----------------------------\n","# 4. SLIDING WINDOW DATASET\n","# -----------------------------\n","class SlidingWindowDataset(torch.utils.data.Dataset):\n","    def __init__(self, tokens, max_length, stride):\n","        self.tokens = tokens\n","        self.max_length = max_length\n","        self.stride = stride\n","\n","        self.inputs = []\n","        self.targets = []\n","\n","        i = 0\n","        while i + max_length < len(tokens):\n","            seq_in = tokens[i:i+max_length]\n","            seq_out = tokens[i+1:i+max_length+1]\n","            self.inputs.append(seq_in)\n","            self.targets.append(seq_out)\n","            i += stride\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n","\n","dataset = SlidingWindowDataset(tokens, max_length, stride)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# -----------------------------\n","# 5. TRANSFORMER BLOCK\n","# -----------------------------\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embedding_dim, num_heads, mlp_hidden_dim):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(embedding_dim)\n","        self.ln2 = nn.LayerNorm(embedding_dim)\n","        self.attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(embedding_dim, mlp_hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_hidden_dim, embedding_dim)\n","        )\n","\n","    def forward(self, x):\n","        # Self-attention with residual\n","        x_norm = self.ln1(x)\n","        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n","        x = x + attn_out\n","\n","        # Feed-forward with residual\n","        x_norm = self.ln2(x)\n","        x = x + self.mlp(x_norm)\n","        return x\n","\n","# -----------------------------\n","# 6. GPT-2 Style Model\n","# -----------------------------\n","class SimpleGPT(nn.Module):\n","    def __init__(self, vocab_size, max_length, embedding_dim, num_heads, mlp_hidden_dim, num_layers):\n","        super().__init__()\n","        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.pos_embedding = nn.Embedding(max_length, embedding_dim)\n","        self.blocks = nn.ModuleList([\n","            TransformerBlock(embedding_dim, num_heads, mlp_hidden_dim) for _ in range(num_layers)\n","        ])\n","        self.ln_f = nn.LayerNorm(embedding_dim)\n","        self.lm_head = nn.Linear(embedding_dim, vocab_size, bias=False)\n","\n","    def forward(self, input_ids):\n","        batch, seq_len = input_ids.shape\n","        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n","        x = self.token_embedding(input_ids) + self.pos_embedding(positions)\n","\n","        for block in self.blocks:\n","            x = block(x)\n","\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)\n","        return logits\n","\n","# -----------------------------\n","# 7. TRAINING\n","# -----------------------------\n","model = SimpleGPT(vocab_size, max_length, embedding_dim, num_heads, mlp_hidden_dim, num_layers)\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch_idx, (input_ids, target_ids) in enumerate(dataloader):\n","        input_ids = input_ids.to(device)\n","        target_ids = target_ids.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(input_ids)                # (batch, seq, vocab_size)\n","        loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        if batch_idx % 10 == 0:\n","            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n","\n","    print(f\"Epoch {epoch+1} finished, Avg Loss: {total_loss/len(dataloader):.4f}\")\n","\n","# -----------------------------\n","# 8. SAVE MODEL LOCALLY\n","# -----------------------------\n","torch.save(model.state_dict(), \"simple_gpt_model.pth\")\n","print(\"Model saved as simple_gpt_model.pth\")\n"]},{"cell_type":"code","source":["# Recreate model architecture (must match the saved model)\n","model = SimpleGPT(vocab_size, max_length, embedding_dim, num_heads, mlp_hidden_dim, num_layers)\n","model.load_state_dict(torch.load(\"simple_gpt_model.pth\", map_location=device))\n","model.to(device)\n","model.eval()  # important for inference (disables dropout etc.)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HKwJNUALUNU","executionInfo":{"status":"ok","timestamp":1764723029841,"user_tz":360,"elapsed":729,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"ee1944af-61fb-4f13-be63-c2a4bc152026"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SimpleGPT(\n","  (token_embedding): Embedding(50257, 768)\n","  (pos_embedding): Embedding(1024, 768)\n","  (blocks): ModuleList(\n","    (0-11): 12 x TransformerBlock(\n","      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=768, out_features=3072, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["prompt = \"She raised her eyebrows\"\n","input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)  # shape: (1, seq_len)\n"],"metadata":{"id":"Ykd9MWh1LXpM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated = input_ids.clone()\n","\n","num_generate = 5\n","for _ in range(num_generate):\n","    with torch.no_grad():\n","        logits = model(generated)             # (1, seq, vocab_size)\n","        next_token_logits = logits[:, -1, :]  # take last token\n","        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # greedy\n","        generated = torch.cat((generated, next_token_id), dim=1)\n"],"metadata":{"id":"BCyadPjoLcO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_text = tokenizer.decode(generated[0].tolist())\n","print(\"Generated text:\\n\", generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5oiKfEoLeja","executionInfo":{"status":"ok","timestamp":1764723278153,"user_tz":360,"elapsed":7,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"cd4449ce-ca90-4c97-9362-2cc6890b2da4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated text:\n"," She raised her eyebrows with its _famille\n"]}]}]}