{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["rWYGL-wPkim3","70RHYgRgvQt_"],"authorship_tag":"ABX9TyORgM9dgowkH/Xthi0DBiKg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Training Loop\n","In this chapter, we implement the training loop and code for basic model evaluation to pretrain an LLM. It follows these steps:\n","\n","- Model generates text\n","- Generated Text is evaluated and loss calculated\n","- Loss is used to calculate the gradiet and readjust the weights.\n","\n","This goes in a loop until, the loss is within the accepted limit and the model predicts with accuracy.\n","\n","![training](https://camo.githubusercontent.com/137f57f6192fbcb6627e6ced1b5274c71924774dec18a4dea29b1c156619ef24/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f30312e77656270)"],"metadata":{"id":"rWYGL-wPkim3"}},{"cell_type":"markdown","source":["### MODEL Code from Previous Chapter\n","I have copied here the MODEL code from the previous chapters so that we can train it."],"metadata":{"id":"gDIOkoOTkmCu"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"id":"rthO0tIuk8uZ","executionInfo":{"status":"ok","timestamp":1764855712181,"user_tz":480,"elapsed":3,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"id":"Slp94esAkVPF","executionInfo":{"status":"ok","timestamp":1764855712225,"user_tz":480,"elapsed":42,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["GPT2_SMALL_CONFIG_124M = {\n","    \"vocab_size\": 50257,        # GPT-2 uses a tokenizer with vocab size of 50,257  (GPT-2 uses Byte Pair Encoder)\n","    \"context_length\": 1024,     # context_length tells you how many tokens you pass in a sequence. also called sequence length.\n","    \"emb_dim\": 768,             # embedding dimension. the vector used by GPT-2 SMALL is of 768 dimension\n","    \"n_heads\": 12,              # number of heads in multi-head attention.\n","    \"n_layers\": 12,             # 12 number of transformer layers/blocks in gpt-2. Each transformer block has 12 multi-head attention.\n","    \"drop_rate\": 0.1,             # dropout rate is used to reduce overfitting\n","    \"qkv_bias\": False           # Query-Key-Value bias\n","}"]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n","        # this will result in errors in the mask creation further below.\n","        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n","        # do not exceed `context_length` before reaching this forward method.\n","\n","        # print(f\"Input Matrix: {x}\\n\")\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # print(f\"Queries Weights: {self.W_query}\\n\")\n","\n","        # print(f\"Queries (after multiplying with weights. 6x3 matrix multiple with 3 x 2 matrix, produces 6 x 2 matrix): {queries}\\n\")\n","\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # print(f\"Queries (after weight split): {queries.shape}\\n\")\n","        # print(f\"Queries (after weight split): {queries}\\n\")\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"dQVOeAotlU6u","executionInfo":{"status":"ok","timestamp":1764855712227,"user_tz":480,"elapsed":1,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n"],"metadata":{"id":"MUtH1brEk7aX","executionInfo":{"status":"ok","timestamp":1764855712229,"user_tz":480,"elapsed":1,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"HKJ0Ijj3lLBT","executionInfo":{"status":"ok","timestamp":1764855712263,"user_tz":480,"elapsed":34,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.dropout(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.dropout(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x"],"metadata":{"id":"rhlOCSUMlZ9U","executionInfo":{"status":"ok","timestamp":1764855712265,"user_tz":480,"elapsed":1,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        # It stacks 12 transformer blocks together\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        # print(f\"Shape of the final normalization layer output: {self.final_norm.shape}\\n\")\n","        # print(f\"the final normalization layer output: {self.final_norm}\\n\")\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","\n","        # embedding + psotional embedding takes place here\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","\n","        # dropout takes place\n","        x = self.drop_emb(x)\n","\n","        # transformer blocks are invoked\n","        x = self.trf_blocks(x)\n","\n","        # final normalization layer\n","        x = self.final_norm(x)\n","\n","        # converts the output to logits which is probability distribution on the vocalbulary (50,257)\n","        logits = self.out_head(x)\n","\n","        return logits"],"metadata":{"id":"jf1XM3D6lj6z","executionInfo":{"status":"ok","timestamp":1764855712267,"user_tz":480,"elapsed":1,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT2_SMALL_CONFIG_124M)"],"metadata":{"id":"x_0JqMICmDiG","executionInfo":{"status":"ok","timestamp":1764855713749,"user_tz":480,"elapsed":1482,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx"],"metadata":{"id":"jG2bu4Q2lvwm","executionInfo":{"status":"ok","timestamp":1764855713752,"user_tz":480,"elapsed":2,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Every effort moves you\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","model.eval() #disables dropout during inference\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT2_SMALL_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1on3tqzlyTc","executionInfo":{"status":"ok","timestamp":1764858998352,"user_tz":480,"elapsed":1219,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"ab58fb5b-6aa1-4209-cecf-d8be3589b9a1"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"]}]},{"cell_type":"markdown","source":["## Model Prediction (without training)\n","Here we will pass a batch of text and see what the model predict. does the prediction match with the target?\n","\n","\n"],"metadata":{"id":"70RHYgRgvQt_"}},{"cell_type":"code","source":["# our input to the model\n","inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n","                       [40,    1107, 588]])   #  \"I really like\"]\n","\n","# expected output from the model\n","targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n","                        [1107,  588, 11311]]) #  \" really like chocolate\"]"],"metadata":{"id":"KGs8aInh5_JM","executionInfo":{"status":"ok","timestamp":1764861198365,"user_tz":480,"elapsed":4,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# model prediction\n","with torch.no_grad():\n","  logits = model(inputs)\n","\n","# pass the logits into softmax to get the probability of each token in the vocabulary\n","probas = torch.softmax(logits, dim=-1)\n","print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFN2orGy6PFM","executionInfo":{"status":"ok","timestamp":1764861199623,"user_tz":480,"elapsed":151,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"cb681a42-16b9-405d-eb2f-b85241581b65"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 50257])\n"]}]},{"cell_type":"code","source":["# For every input token, let's find the target token with max probability\n","# Inputs: [[\"every effort moves\"], [\"I really like\"]]\n","token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n","print(\"Token IDs:\\n\", token_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYYL5PcF7QA3","executionInfo":{"status":"ok","timestamp":1764861274991,"user_tz":480,"elapsed":17,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"9f25d435-1959-4a32-e15b-60f6762bae77"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Token IDs:\n"," tensor([[[36397],\n","         [39619],\n","         [20610]],\n","\n","        [[ 8615],\n","         [49289],\n","         [47105]]])\n"]}]},{"cell_type":"markdown","source":["The output token ids should gives the below. that's our expectation.\n","```\n","Token IDs Expected:\n"," tensor([[[3626],  --> effort\n","         [6100],   --> moves\n","         [345]],  --> you\n","\n","        [[1107],   --> really\n","         [588],   --> like\n","         [11311]]]) --> chocolate\n","```\n","\n","```\n","Token IDs that the model predicted:\n"," tensor([[[36397],  --> Gathering\n","         [39619],   --> Serbian\n","         [20610]],  --> Friday\n","\n","        [[ 8615],   --> cos\n","         [49289],   --> slicing\n","         [47105]]]) --> Aux\n","```\n","\n","\n","Let's deocde them to TEXT and see."],"metadata":{"id":"SwB1E0Ai7nx2"}},{"cell_type":"code","source":["print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n","print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\\n\")\n","\n","print(f\"Targets batch 2: {token_ids_to_text(targets[1], tokenizer)}\")\n","print(f\"Outputs batch 2: {token_ids_to_text(token_ids[1].flatten(), tokenizer)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5BvRg0u8JMV","executionInfo":{"status":"ok","timestamp":1764861482810,"user_tz":480,"elapsed":11,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"6f89d98e-0c58-4b45-a5d4-cac5283cb73e"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Targets batch 1:  effort moves you\n","Outputs batch 1:  Gathering SerbianFriday\n","\n","Targets batch 2:  really like chocolate\n","Outputs batch 2:  cos slicing Aux\n"]}]},{"cell_type":"markdown","source":["## Cross-Entropy Loss"],"metadata":{"id":"3rMIAkrl-Bf6"}}]}