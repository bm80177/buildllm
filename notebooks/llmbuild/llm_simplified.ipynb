{"cells":[{"cell_type":"markdown","source":["# Stage1: Preprocessing\n","\n"],"metadata":{"id":"Bh70pARGxSi-"},"id":"Bh70pARGxSi-"},{"cell_type":"markdown","id":"d35b7620","metadata":{"id":"d35b7620"},"source":["## 1. Tokenization and Encoding\n","\n","**Tokenization and Encoding** is the very first step in the LLM where the input text is broken down into tokens, and tokens are translated into integers. GPT-2 uses BytePairEncoding (BPE) as tokenizer.\n","Let's see how it works."]},{"cell_type":"code","execution_count":null,"id":"5a85cec7","metadata":{"id":"5a85cec7","outputId":"fb773548-c917-4de9-f79b-43ef7e763762"},"outputs":[{"name":"stdout","output_type":"stream","text":["tiktoken version: 0.12.0\n"]}],"source":["import importlib\n","import tiktoken # OpenAI's tokenizer library\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"]},{"cell_type":"code","execution_count":null,"id":"b86e8f35","metadata":{"id":"b86e8f35","outputId":"4b3e9d08-e59b-42d1-9d0d-de5fd1fcd632"},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded into Integers:\n"," [15496, 11, 466, 345, 588, 8887, 13, 1148, 428, 438, 257, 1332, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n","\n","\n","Decoded into Text:\n"," Hello, do you like tea. Is this-- a test? <|endoftext|> In the sunlit terraces of the palace.\n"]}],"source":["text1 = \"Hello, do you like tea. Is this-- a test?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","\n","# Add <|endoftext|> as line breaker.\n","text = \" <|endoftext|> \".join((text1, text2))\n","\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","\n","print(f\"Encoded into Integers:\\n {integers}\")\n","print(\"\\n\")\n","\n","strings = tokenizer.decode(integers)\n","\n","print(f\"Decoded into Text:\\n {strings}\")"]},{"cell_type":"markdown","id":"ec095f51","metadata":{"id":"ec095f51"},"source":["### Sampling / Tensors with Sliding Window\n","The way the LLM works is that it always predicts the next word. It means, it must be trained in the same manner.\n","\n","E.g. Lets say we have to train the LLM on one TEXT line \"LLM learns to predict one word at a time\".\n","\n","We will have to break this one TEXT/Chunk line into multiple input sub-chunks, and output as shown below for a context window of 3.\n","\n","|Input Tokens|Expected Output Token|\n","|---|---|\n","|LLM learns to|learns to predict|\n","|learns to predict|to predict one|\n","|to predict one|predict one word|\n","|predict one word|one word at|\n","|one word at|word at a|\n","|word at a|at a time|\n","\n","Also to note is that the input could be a very long TEXT which not might fit into the model's context window. When training a model like GPT-2, the model can only process a limited number of tokens at a time (for example, 1024 is the context window of gpt-2). So we must split the long text into manageable chunks (tensors). The context window is also called **max_length**. **stride**: Controls how much overlap there is between consecutive training chunks.\n","\n","```\n","Suppose:\n","max_length = 10\n","stride = 5\n","and your tokens are [t1, t2, t3, â€¦, t25].\n","\n","Then your dataset chunks will be:\n","Input Chunk 1: t1  to t10                                   -->  Output Chunk 1:                         \n","Input Chunk 2: t6  to t15  (an overlap from t6 to t10)      \n","Input Chunk 3: t11 to t20  (an overlap from t11 to t115)    \n","Input Chunk 4: t16 to t25  (an overlap from t16 to t20)    \n","```\n","\n","This is achieved through data loader and dataset, that breaks the input texts into chunks.\n"]},{"cell_type":"code","execution_count":null,"id":"97df46f1","metadata":{"id":"97df46f1"},"outputs":[],"source":["import importlib\n","import tiktoken\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","class SlidingWindowDataset(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    ## max_length --> how many tokens will be placed in each tensor\n","    ## stride --> ??\n","    dataset = SlidingWindowDataset(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,   # number of samples/chunks per batch\n","        shuffle=shuffle,         # whether to shuffle the data at every epoch\n","        drop_last=drop_last,     # true means that the last batch is dropped if it's smaller than batch_size\n","        num_workers=num_workers  # >0 is used for parallel data loading\n","    )\n","\n","    return dataloader"]},{"cell_type":"code","execution_count":null,"id":"7dd1ef1b","metadata":{"id":"7dd1ef1b","outputId":"c15d70f4-8b88-4a0c-b218-b2143725a9e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["First Batch:\n","------------------------------------\n","\n","Input Tensor: tensor([[  464,  4252, 18859,  2157,   262],\n","        [18859,  2157,   262, 18639,   355],\n","        [  262, 18639,   355,   262,  1633]])\n","Decoded Tensor Seq1: The sun slipped behind the\n","Decoded Tensor Seq2  slipped behind the hills as\n","Decoded Tensor Seq3:  the hills as the air\n","\n","\n","Expected Output Tensor: tensor([[ 4252, 18859,  2157,   262, 18639],\n","        [ 2157,   262, 18639,   355,   262],\n","        [18639,   355,   262,  1633,  2900]])\n","Decoded Tensor Seq1 expected output:  sun slipped behind the hills\n","Decoded Tensor Seq2 expected output:  behind the hills as the\n","Decoded Tensor Seq3 expected output:  hills as the air turned\n","\n","\n","Second Batch:\n","------------------------------------\n","\n","Input Tensor: tensor([[  355,   262,  1633,  2900, 38427],\n","        [ 1633,  2900, 38427,    13,   317],\n","        [38427,    13,   317, 28583,  3290]])\n","Decoded Tensor Seq1:  as the air turned colder\n","Decoded Tensor Seq2:  air turned colder. A\n","Decoded Tensor Seq3:  colder. A stray dog\n","\n","\n","Expected Output Tensor: tensor([[  262,  1633,  2900, 38427,    13],\n","        [ 2900, 38427,    13,   317, 28583],\n","        [   13,   317, 28583,  3290, 36036]])\n","Decoded Tensor Seq1 expected output:  the air turned colder.\n","Decoded Tensor Seq2 expected output:  turned colder. A stray\n","Decoded Tensor Seq3 expected output: . A stray dog wandered\n","\n","\n"]}],"source":["batch_size=3\n","max_length=5\n","stride=2\n","\n","# with open(\"../../_data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","#     raw_text = f.read()\n","\n","raw_text = \"The sun slipped behind the hills as the air turned colder. A stray dog wandered along the empty street searching for food.\"\n","\n","# It tokenizes the raw texts into token integers first using BytePairEncoder\n","# It then breaks the token arrary to chunks or smaller token arrays or tensors\n","dataloader = create_dataloader_v1(\n","    raw_text, batch_size=batch_size, max_length=max_length, stride=stride, shuffle=False\n",")\n","\n","data_iter = iter(dataloader)\n","print(\"First Batch:\")\n","print(\"------------------------------------\\n\")\n","input1, target1 = next(data_iter)\n","print(\"Input Tensor:\", input1)\n","print(\"Decoded Tensor Seq1:\", tokenizer.decode(input1[0].tolist()))\n","print(\"Decoded Tensor Seq2\", tokenizer.decode(input1[1].tolist()))\n","print(\"Decoded Tensor Seq3:\", tokenizer.decode(input1[2].tolist()))\n","print(\"\\n\")\n","print(\"Expected Output Tensor:\", target1)\n","print(\"Decoded Tensor Seq1 expected output:\", tokenizer.decode(target1[0].tolist()))\n","print(\"Decoded Tensor Seq2 expected output:\", tokenizer.decode(target1[1].tolist()))\n","print(\"Decoded Tensor Seq3 expected output:\", tokenizer.decode(target1[2].tolist()))\n","print(\"\\n\")\n","\n","print(\"Second Batch:\")\n","print(\"------------------------------------\\n\")\n","input2, target2 = next(data_iter)\n","print(\"Input Tensor:\", input2)\n","print(\"Decoded Tensor Seq1:\", tokenizer.decode(input2[0].tolist()))\n","print(\"Decoded Tensor Seq2:\", tokenizer.decode(input2[1].tolist()))\n","print(\"Decoded Tensor Seq3:\", tokenizer.decode(input2[2].tolist()))\n","print(\"\\n\")\n","print(\"Expected Output Tensor:\", target2)\n","print(\"Decoded Tensor Seq1 expected output:\", tokenizer.decode(target2[0].tolist()))\n","print(\"Decoded Tensor Seq2 expected output:\", tokenizer.decode(target2[1].tolist()))\n","print(\"Decoded Tensor Seq3 expected output:\", tokenizer.decode(target2[2].tolist()))\n","print(\"\\n\")"]},{"cell_type":"markdown","id":"5d89d8a3","metadata":{"id":"5d89d8a3"},"source":["## 2. Embedding\n","Now that you have the token tensors (input tensors + expected output tensors) produced with sliding windows, the next step is to vectorize them."]},{"cell_type":"code","execution_count":null,"id":"9fd81b47","metadata":{"id":"9fd81b47"},"outputs":[],"source":["import torch\n","import tiktoken #BytePairEncoder.\n","\n","vocab_size = tokenizer.n_vocab  # Because we used tiktoken for encoding, we should use its vocabulary size.\n","                                # The BytePair encoder has a vocabulary size of 50,257\n","embedding_dim = 768             # Vector matrix dimension of GPT-2 is 768\n","# The embedding layer coverts each token into 768 dimension vector.\n","token_embedding_layer = torch.nn.Embedding(\n","    num_embeddings=vocab_size,      # size of the vocabulary. That is, total number of unique tokens\n","    embedding_dim=embedding_dim     # dimension of the embedding vector for each token\n",")"]},{"cell_type":"code","execution_count":null,"id":"df6462e6","metadata":{"id":"df6462e6","outputId":"054e79b1-e31c-428e-f374-49b5e0bcab18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input is a 1D tensor: torch.Size([4])\n","Output 2D tensor size: torch.Size([4, 768]). Each token is converted to a 768 dimension vector.\n","tensor([[ 0.8774, -1.4247,  0.2718,  ...,  1.3046, -0.4417,  0.8319],\n","        [ 0.1590,  0.2387, -2.3928,  ..., -1.3388,  0.3896, -1.4132],\n","        [ 0.2799, -1.0826, -0.4743,  ...,  1.6107, -0.6543, -0.4180],\n","        [ 0.3794, -0.0688, -0.6316,  ...,  1.9465, -1.0704, -0.2395]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["# Example token IDs. Let says, we have tensor of 4 tokens only\n","# Each token must be converted into vector.\n","# So, it will be 4 x 768 matrix\n","token_ids = torch.tensor([15496, 11, 703, 389])\n","token_ids.shape\n","\n","# Get embeddings\n","print(f'Input is a 1D tensor: {token_ids.shape}')\n","embeddings = token_embedding_layer(token_ids)\n","print(f\"Output 2D tensor size: {embeddings.shape}. Each token is converted to a 768 dimension vector.\")\n","# print(token_embedding_layer.weight)\n","print(embeddings)"]},{"cell_type":"code","execution_count":null,"id":"0cf6d6cd","metadata":{"id":"0cf6d6cd","outputId":"c53fede1-3c2f-416d-f116-10d2216f894a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","===== Embeddings for First Batch =====\n","\n","Input is a 1D tensor: torch.Size([3, 5])\n","Output 2D tensor size: torch.Size([3, 5, 768]). Each token is converted to a 768 dimension vector.\n","tensor([[[-0.5329, -1.3247, -0.1321,  ...,  0.7444,  1.9264,  1.4444],\n","         [-0.5785, -1.3702,  1.0191,  ...,  0.5402,  0.3909,  0.6542],\n","         [ 0.7275,  0.6341,  1.9347,  ..., -0.9212,  0.2855,  0.0110],\n","         [ 0.1893, -0.4733, -0.9623,  ...,  0.0555, -0.9752, -0.4528],\n","         [-0.2465,  0.3179, -0.5723,  ...,  0.0317, -0.9859,  0.3813]],\n","\n","        [[ 0.7275,  0.6341,  1.9347,  ..., -0.9212,  0.2855,  0.0110],\n","         [ 0.1893, -0.4733, -0.9623,  ...,  0.0555, -0.9752, -0.4528],\n","         [-0.2465,  0.3179, -0.5723,  ...,  0.0317, -0.9859,  0.3813],\n","         [ 0.1323,  1.0493, -0.7841,  ...,  0.0362, -0.3318, -0.1653],\n","         [-0.6942, -1.4064, -1.7145,  ...,  0.0332, -1.1418,  0.0596]],\n","\n","        [[-0.2465,  0.3179, -0.5723,  ...,  0.0317, -0.9859,  0.3813],\n","         [ 0.1323,  1.0493, -0.7841,  ...,  0.0362, -0.3318, -0.1653],\n","         [-0.6942, -1.4064, -1.7145,  ...,  0.0332, -1.1418,  0.0596],\n","         [-0.2465,  0.3179, -0.5723,  ...,  0.0317, -0.9859,  0.3813],\n","         [-1.5276,  0.1562, -1.2402,  ...,  0.1709, -0.6681, -0.0050]]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["# ---------------------------------------------------------\n","# Pass batch1 token IDs through embeddings\n","# ---------------------------------------------------------\n","print(\"\\n===== Embeddings for First Batch =====\\n\")\n","\n","# Shape: [batch_size, max_length, embedding_dim]\n","print(f'Input is a 1D tensor: {input1.shape}')\n","batch1_embeddings = token_embedding_layer(input1)\n","\n","print(f\"Output 2D tensor size: {batch1_embeddings.shape}. Each token is converted to a 768 dimension vector.\")\n","print(batch1_embeddings)"]},{"cell_type":"markdown","id":"dd919c81","metadata":{"id":"dd919c81"},"source":["## 3. Add Positional Embedding\n","\n","In an input sequence like \"fox jumps over the fox\", the encoder is going to generate the same token id. The same token id will translates into same embedding vectors. Now, the concern is how will you distinguish between, the fox in the begining of the sequence from the fox at the end of the sequence.\n","\n","This is where positional embedding plays a role. It defines an embedding for each position in the window context.\n","\n","Say, GPT-2 has a context window of 1024 token. that's the maximum, it can process. Each token is a 768 dimension vector.\n","\n","It means, every sequence input to GPT-2 is a tensor of 1024 x 768 size.\n","\n","The positional embedding is also of same size 1024 x 768, and it has a FIXED set of values for every GPT. It does not change with the input text or embeddings.\n","\n","The positional embedding is simply added to the input embedding to produce the input to the TRANSFORMER.\n","\n","In our example here, we are using context window (max_length) as 5 for simplicity. While building the final GPT-2, we will use it as 1024."]},{"cell_type":"code","execution_count":null,"id":"3cf91fcf","metadata":{"id":"3cf91fcf"},"outputs":[],"source":["import torch\n","\n","embedding_dim = 768             # Vector matrix dimension of GPT-2 is 768\n","# The embedding layer coverts each token into 768 dimension vector.\n","position_embedding_layer = torch.nn.Embedding(\n","    num_embeddings=max_length,      # this is the total number of tokens you can pass in a sequence.\n","                                    # It is also called context window. Its 1024 for GPT-2\n","    embedding_dim=embedding_dim     # dimension of the embedding vector for each token\n",")"]},{"cell_type":"code","execution_count":null,"id":"ba9b492a","metadata":{"id":"ba9b492a","outputId":"bae17f8d-4b55-4ca6-ab06-4042785a53c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 768])\n"]}],"source":["pos_embeddings = position_embedding_layer(torch.arange(max_length))\n","print(pos_embeddings.shape)"]},{"cell_type":"code","execution_count":null,"id":"50e0636a","metadata":{"id":"50e0636a","outputId":"b2342460-261f-4f14-9b5c-ce331498b153"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[ 1.4296, -1.4895, -1.0155,  ...,  0.7876,  5.0135,  0.6227],\n","         [-1.6781, -4.3282,  2.4288,  ...,  1.0637, -0.4990,  1.4549],\n","         [ 1.9781,  1.8971,  2.1632,  ..., -1.4088,  1.9509,  0.2565],\n","         [ 1.1339,  0.0960, -2.1792,  ..., -2.0248, -0.6697,  0.8989],\n","         [ 1.4439,  0.8378, -2.0558,  ...,  0.3818, -0.3582,  1.0698]],\n","\n","        [[ 2.6900,  0.4693,  1.0513,  ..., -0.8781,  3.3726, -0.8107],\n","         [-0.9103, -3.4313,  0.4474,  ...,  0.5790, -1.8651,  0.3478],\n","         [ 1.0041,  1.5809, -0.3437,  ..., -0.4559,  0.6796,  0.6268],\n","         [ 1.0769,  1.6185, -2.0009,  ..., -2.0440, -0.0263,  1.1864],\n","         [ 0.9962, -0.8865, -3.1980,  ...,  0.3833, -0.5142,  0.7481]],\n","\n","        [[ 1.7160,  0.1531, -1.4557,  ...,  0.0749,  2.1012, -0.4403],\n","         [-0.9673, -1.9087,  0.6257,  ...,  0.5597, -1.2217,  0.6353],\n","         [ 0.5564, -0.1434, -1.4859,  ..., -0.4544,  0.5236,  0.3051],\n","         [ 0.6981,  0.8871, -1.7891,  ..., -2.0486, -0.6804,  1.7331],\n","         [ 0.1628,  0.6761, -2.7238,  ...,  0.5210, -0.0405,  0.6834]]],\n","       grad_fn=<AddBackward0>)\n"]}],"source":["# Input Embeddings = token embeddings + positional encodings\n","input_embeddings_batch1 = batch1_embeddings + pos_embeddings\n","\n","print(input_embeddings_batch1)"]},{"cell_type":"markdown","id":"a8d4b54f","metadata":{"id":"a8d4b54f"},"source":["## 4. Summary\n","The final out of the 3 steps (tokenization, embedding, and positional embedding) is a batch of sequences.\n","\n","- A batch has \"batch_size\" no of sequences.\n","- Each sequence has \"max_length\" no of tokens. (this is nothing but the context window of the llm. gpt-2 context window is 1024)\n","- Each token has \"vector_dimension\" of embeddings.\n","\n","**Basically, it produces an array of tensors. And each tensor is of [batch_size x max_length x vector_dimension] size.** The vector dimension in gpt-2 is 768. It means, each vector within gpt is represented in 768 dimensions.\n","\n"]},{"cell_type":"markdown","source":["# Stage1: Attention"],"metadata":{"id":"2yPYHdV5yijJ"},"id":"2yPYHdV5yijJ"}],"metadata":{"kernelspec":{"display_name":"Python 3.12 (.venv)","language":"python","name":"buildllm-venv"},"colab":{"provenance":[],"collapsed_sections":["d35b7620","5d89d8a3"]}},"nbformat":4,"nbformat_minor":5}