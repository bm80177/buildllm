{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiJJwqgUn85e"
   },
   "source": [
    "# A Typical Machine Learning Workflow\n",
    "\n",
    "Machine learning typically occurs in a **loop** where you adjust the model‚Äôs weights and bias, measure performance, and iteratively improve the model until it is finalized.\n",
    "\n",
    "The training loop generally consists of **four main steps**:\n",
    "\n",
    "1. **Prediction (Forward Pass)**  \n",
    "   Compute the model‚Äôs output based on the current weights and input data.\n",
    "\n",
    "2. **Loss Computation**  \n",
    "   Measure how far the predictions are from the true values using a **loss function**.\n",
    "\n",
    "3. **Gradient Calculation (Backward Pass)**  \n",
    "   Compute gradients of the loss with respect to model parameters using **automatic differentiation**.\n",
    "\n",
    "4. **Parameter Updates**  \n",
    "   Adjust the weights and bias in the direction that minimizes the loss (e.g., using **gradient descent**).\n",
    "\n",
    "\n",
    "With Examples, We will learn how to move from manually performing all these steps to automated way using **pytorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOoHgwKMs3pI"
   },
   "source": [
    "# Example1 (performing all steps manually)\n",
    "This is to undestand how all the machine learning steps execute. We are using a simple linear regression f = w * x for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1762362355004,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 480
    },
    "id": "dvwnny7ApIBL",
    "outputId": "d16d2130-0ca7-4e32-9c82-130b9d11b6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 01: w = 1.200, loss = 30.00000000\n",
      "Epoch 06: w = 1.992, loss = 0.00314574\n",
      "Epoch 11: w = 2.000, loss = 0.00000033\n",
      "Epoch 16: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üìò Simple Linear Regression from Scratch using NumPy\n",
    "---------------------------------------------------\n",
    "This script demonstrates how machine learning training works internally\n",
    "‚Äî without using frameworks like PyTorch or TensorFlow.\n",
    "\n",
    "We model a simple linear relationship:\n",
    "    y = w * x\n",
    "\n",
    "Goal:\n",
    "    Learn the optimal value of weight `w` that minimizes Mean Squared Error (MSE)\n",
    "    between predicted and true values.\n",
    "\n",
    "Steps:\n",
    "    1. Initialize parameters\n",
    "    2. Forward pass (prediction)\n",
    "    3. Compute loss (MSE)\n",
    "    4. Compute gradient (slope of loss wrt w)\n",
    "    5. Update weights (gradient descent)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1Ô∏è‚É£ Training Data\n",
    "# ---------------------------------------------------\n",
    "# True relationship: y = 2x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32) # Ytrue\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2Ô∏è‚É£ Initialize Model Parameter\n",
    "# ---------------------------------------------------\n",
    "# Start with an arbitrary weight\n",
    "w = 0.0  # initial guess for slope\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3Ô∏è‚É£ Define Core Functions\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def forward(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Forward pass: Predicts the output for given input `x`\n",
    "    using the current weight `w`.\n",
    "    Equation: y_pred = w * x\n",
    "    \"\"\"\n",
    "    return w * x\n",
    "\n",
    "\n",
    "def loss(y: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes Mean Squared Error (MSE)\n",
    "    Equation: MSE = (1/N) * Œ£(y_pred - y)^2\n",
    "    \"\"\"\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "\n",
    "def gradient(x: np.ndarray, y: np.ndarray, y_predicted: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss function\n",
    "    with respect to weight `w`.\n",
    "\n",
    "    Derivation:\n",
    "        MSE = (1/N) * Œ£(w*x - y)^2\n",
    "        dMSE/dw = (2/N) * Œ£(x * (w*x - y))\n",
    "    \"\"\"\n",
    "    return np.dot(2 * x, y_predicted - y).mean()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4Ô∏è‚É£ Before Training ‚Äî Initial Prediction\n",
    "# ---------------------------------------------------\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5Ô∏è‚É£ Training Loop (Gradient Descent)\n",
    "# ---------------------------------------------------\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # üß≠ Forward Pass ‚Äî Compute Predictions\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # üìâ Compute Loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # üßÆ Compute Gradient (how loss changes with w)\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # ‚öôÔ∏è Update Weight ‚Äî Gradient Descent Step\n",
    "    # new_w = old_w - learning_rate * gradient\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    # ü™Ñ Log Progress Every 10 Epochs\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:02d}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6Ô∏è‚É£ After Training ‚Äî Evaluate Model\n",
    "# ---------------------------------------------------\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb2EBEA7tcdx"
   },
   "source": [
    "# Example2 (partially automated)\n",
    "\n",
    "In stead of using numpy, we will use **tensor** from pytorch.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "By defining weight(w) as a tensor with \"requires_grad=True\", we are asking the pytorch **Autograd** to build the computation graph, do the gradient calculation.\n",
    "\n",
    ".backward() calculates the gradient, and updates the parameter .grad property with the value. We do not need the the custom gradient() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1762362355084,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 480
    },
    "id": "nckvW2rruCIT",
    "outputId": "5c4ec018-e203-4832-b873-656a74d9adfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 001: w = 0.300, loss = 30.00000000\n",
      "Epoch 011: w = 1.665, loss = 1.16278565\n",
      "Epoch 021: w = 1.934, loss = 0.04506890\n",
      "Epoch 031: w = 1.987, loss = 0.00174685\n",
      "Epoch 041: w = 1.997, loss = 0.00006770\n",
      "Epoch 051: w = 1.999, loss = 0.00000262\n",
      "Epoch 061: w = 2.000, loss = 0.00000010\n",
      "Epoch 071: w = 2.000, loss = 0.00000000\n",
      "Epoch 081: w = 2.000, loss = 0.00000000\n",
      "Epoch 091: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üìò Simple Linear Regression using PyTorch (Manual Training Loop)\n",
    "---------------------------------------------------------------\n",
    "This script demonstrates how PyTorch automates gradient computation\n",
    "using its **autograd** engine.\n",
    "\n",
    "We‚Äôre modeling the relationship:\n",
    "    y = 2 * x\n",
    "\n",
    "Goal:\n",
    "    Learn the optimal weight `w` such that predictions match the true values y.\n",
    "\n",
    "üöÄ Key Improvement Over Manual NumPy Version:\n",
    "---------------------------------------------\n",
    "‚úÖ In the NumPy version, we manually computed the gradient (dL/dw).\n",
    "‚úÖ In this PyTorch version, **autograd automatically calculates gradients**\n",
    "   during the backward pass using `loss.backward()` ‚Äî eliminating the need\n",
    "   for manual differentiation.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1Ô∏è‚É£ Define Training Data\n",
    "# ---------------------------------------------------\n",
    "# True relationship: y = 2x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2Ô∏è‚É£ Initialize Model Parameter\n",
    "# ---------------------------------------------------\n",
    "# requires_grad=True lets PyTorch track operations on w for autograd\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3Ô∏è‚É£ Define Core Functions\n",
    "# ---------------------------------------------------\n",
    "def forward(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward pass: Predict output using the current weight.\n",
    "    Equation: y_pred = w * x\n",
    "    \"\"\"\n",
    "    return w * x\n",
    "\n",
    "\n",
    "def loss(y: torch.Tensor, y_predicted: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error (MSE) loss.\n",
    "    MSE = (1/N) * Œ£(y_pred - y)^2\n",
    "    \"\"\"\n",
    "    return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4Ô∏è‚É£ Before Training ‚Äî Check Initial Prediction\n",
    "# ---------------------------------------------------\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5Ô∏è‚É£ Training Configuration\n",
    "# ---------------------------------------------------\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6Ô∏è‚É£ Training Loop\n",
    "# ---------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # üß≠ Forward Pass ‚Äî Compute Predictions\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # üìâ Compute Loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # üßÆ Backward Pass ‚Äî Autograd computes dL/dw automatically and stores in w.grad\n",
    "    l.backward()\n",
    "\n",
    "    # ‚öôÔ∏è Update Weight ‚Äî Gradient Descent Step. no_grad() ensure, the weight updated is not added to the w tensor computational graph.\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # üßπ Reset gradients to zero for the next iteration.\n",
    "    w.grad.zero_()\n",
    "\n",
    "    # ü™Ñ Log progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7Ô∏è‚É£ After Training ‚Äî Evaluate Model\n",
    "# ---------------------------------------------------\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiPIYbdrxqdr"
   },
   "source": [
    "# Example3 (fully automated)\n",
    "\n",
    "Here we will automate the previous example further:\n",
    "1. Will use the pytorch linear model that would replace the forward() method.\n",
    "2. Replace the our loss & optimizer calculation with pytorch loss and optimizer methods.\n",
    "\n",
    "**Note:**\n",
    "The X & Y are reshaped to 4 x 1 matrix. That's how the pytorch model expects input. Every input shall be treated as a row in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1762362355239,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 480
    },
    "id": "nOWBiua51Cje",
    "outputId": "60a47b1d-0d6e-412e-ef15-eef84270906b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 4, Features: 1\n",
      "Prediction before training: f(5) = -3.002\n",
      "Epoch 001: w = -0.400, b = 0.984, loss = 46.45131302\n",
      "Epoch 021: w = 1.435, b = 1.503, loss = 0.41872454\n",
      "Epoch 041: w = 1.512, b = 1.431, loss = 0.34410033\n",
      "Epoch 061: w = 1.542, b = 1.348, loss = 0.30519095\n",
      "Epoch 081: w = 1.568, b = 1.269, loss = 0.27069762\n",
      "Epoch 101: w = 1.593, b = 1.195, loss = 0.24010263\n",
      "Epoch 121: w = 1.617, b = 1.126, loss = 0.21296556\n",
      "Epoch 141: w = 1.639, b = 1.060, loss = 0.18889576\n",
      "Epoch 161: w = 1.660, b = 0.999, loss = 0.16754624\n",
      "Epoch 181: w = 1.680, b = 0.940, loss = 0.14860973\n",
      "Epoch 201: w = 1.699, b = 0.886, loss = 0.13181348\n",
      "Epoch 221: w = 1.716, b = 0.834, loss = 0.11691565\n",
      "Epoch 241: w = 1.733, b = 0.786, loss = 0.10370149\n",
      "Epoch 261: w = 1.748, b = 0.740, loss = 0.09198096\n",
      "Epoch 281: w = 1.763, b = 0.697, loss = 0.08158500\n",
      "Epoch 301: w = 1.777, b = 0.656, loss = 0.07236403\n",
      "Epoch 321: w = 1.790, b = 0.618, loss = 0.06418534\n",
      "Epoch 341: w = 1.802, b = 0.582, loss = 0.05693090\n",
      "Epoch 361: w = 1.814, b = 0.548, loss = 0.05049643\n",
      "Epoch 381: w = 1.824, b = 0.516, loss = 0.04478922\n",
      "Prediction after training: f(5) = 9.658\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üìò Linear Regression using PyTorch (High-Level API)\n",
    "--------------------------------------------------\n",
    "This script demonstrates how PyTorch‚Äôs built-in modules\n",
    "automate most of the training process for a linear regression model.\n",
    "\n",
    "We‚Äôre modeling the relationship:\n",
    "    y = 2 * x\n",
    "\n",
    "Goal:\n",
    "    Learn the optimal weight (w) and bias (b) that best fit the data.\n",
    "\n",
    "üöÄ Key Improvement Over Previous Version:\n",
    "-----------------------------------------\n",
    "‚úÖ In the earlier version, we manually defined the forward pass,\n",
    "   computed the loss, and updated the parameter `w` using autograd.\n",
    "‚úÖ In this version, **PyTorch automates everything**:\n",
    "   - `nn.Linear` handles model definition (w and b)\n",
    "   - `nn.MSELoss` handles loss computation\n",
    "   - `torch.optim.SGD` handles weight updates\n",
    "\n",
    "This represents a fully modular, high-level approach compared to\n",
    "manual gradient descent.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1Ô∏è‚É£ Define Training and Test Data\n",
    "# ---------------------------------------------------\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# Extract input/output sizes\n",
    "n_samples, n_features = X.shape\n",
    "print(f\"Samples: {n_samples}, Features: {n_features}\")\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2Ô∏è‚É£ Define Model\n",
    "# ---------------------------------------------------\n",
    "# nn.Linear automatically creates weight and bias parameters\n",
    "model = nn.Linear(in_features=input_size,\n",
    "                  out_features=output_size,\n",
    "                  bias=True)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3Ô∏è‚É£ Define Loss and Optimizer\n",
    "# ---------------------------------------------------\n",
    "# Mean Squared Error (MSE) loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Stochastic Gradient Descent optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4Ô∏è‚É£ Training Loop\n",
    "# ---------------------------------------------------\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # üß≠ Forward Pass ‚Äî Generate Predictions\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # üìâ Compute Loss\n",
    "    l = criterion(Y, y_pred)\n",
    "\n",
    "    # üßÆ Backward Pass ‚Äî Compute Gradients Automatically\n",
    "    l.backward()\n",
    "\n",
    "    # ‚öôÔ∏è Update Weights and Bias (handled by optimizer)\n",
    "    optimizer.step()\n",
    "\n",
    "    # üßπ Reset gradients before next iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ü™Ñ Log progress every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f\"Epoch {epoch+1:03d}: w = {w[0][0].item():.3f}, b = {b.item():.3f}, loss = {l:.8f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5Ô∏è‚É£ After Training ‚Äî Evaluate Model\n",
    "# ---------------------------------------------------\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjF6gu6s96Mh"
   },
   "source": [
    "# Example4 (defining custom model)\n",
    "\n",
    "Here we will define a custom linear model and run the same demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1762362355437,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 480
    },
    "id": "tKohRd00-HvW",
    "outputId": "525888d8-2cad-41ef-e5a3-8871564dc466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 4, Features: 1\n",
      "Prediction before training: f(5) = 0.942\n",
      "Epoch 001: w = 0.472, b = 0.038, loss = 24.80678368\n",
      "Epoch 021: w = 1.805, b = 0.458, loss = 0.05376185\n",
      "Epoch 041: w = 1.848, b = 0.443, loss = 0.03299318\n",
      "Epoch 061: w = 1.858, b = 0.417, loss = 0.02925432\n",
      "Epoch 081: w = 1.866, b = 0.393, loss = 0.02594792\n",
      "Epoch 101: w = 1.874, b = 0.370, loss = 0.02301523\n",
      "Epoch 121: w = 1.881, b = 0.349, loss = 0.02041401\n",
      "Epoch 141: w = 1.888, b = 0.328, loss = 0.01810675\n",
      "Epoch 161: w = 1.895, b = 0.309, loss = 0.01606024\n",
      "Epoch 181: w = 1.901, b = 0.291, loss = 0.01424512\n",
      "Epoch 201: w = 1.907, b = 0.274, loss = 0.01263509\n",
      "Epoch 221: w = 1.912, b = 0.258, loss = 0.01120705\n",
      "Epoch 241: w = 1.917, b = 0.243, loss = 0.00994039\n",
      "Epoch 261: w = 1.922, b = 0.229, loss = 0.00881692\n",
      "Epoch 281: w = 1.927, b = 0.216, loss = 0.00782038\n",
      "Epoch 301: w = 1.931, b = 0.203, loss = 0.00693652\n",
      "Epoch 321: w = 1.935, b = 0.191, loss = 0.00615253\n",
      "Epoch 341: w = 1.939, b = 0.180, loss = 0.00545717\n",
      "Epoch 361: w = 1.942, b = 0.170, loss = 0.00484038\n",
      "Epoch 381: w = 1.946, b = 0.160, loss = 0.00429330\n",
      "Prediction after training: f(5) = 9.894\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üìò Linear Regression using PyTorch (High-Level API)\n",
    "--------------------------------------------------\n",
    "This script demonstrates how PyTorch‚Äôs built-in modules\n",
    "automate most of the training process for a linear regression model.\n",
    "\n",
    "We‚Äôre modeling the relationship:\n",
    "    y = 2 * x\n",
    "\n",
    "Goal:\n",
    "    Learn the optimal weight (w) and bias (b) that best fit the data.\n",
    "\n",
    "üöÄ Key Improvement Over Previous Version:\n",
    "-----------------------------------------\n",
    "‚úÖ In the earlier version, we manually defined the forward pass,\n",
    "   computed the loss, and updated the parameter `w` using autograd.\n",
    "‚úÖ In this version, **PyTorch automates everything**:\n",
    "   - `nn.Linear` handles model definition (w and b)\n",
    "   - `nn.MSELoss` handles loss computation\n",
    "   - `torch.optim.SGD` handles weight updates\n",
    "\n",
    "This represents a fully modular, high-level approach compared to\n",
    "manual gradient descent.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1Ô∏è‚É£ Define Training and Test Data\n",
    "# ---------------------------------------------------\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# Extract input/output sizes\n",
    "n_samples, n_features = X.shape\n",
    "print(f\"Samples: {n_samples}, Features: {n_features}\")\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2Ô∏è‚É£ Define Model\n",
    "# ---------------------------------------------------\n",
    "# nn.Linear automatically creates weight and bias parameters\n",
    "# model = nn.Linear(in_features=input_size,\n",
    "#                   out_features=output_size,\n",
    "#                   bias=True)\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define our layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3Ô∏è‚É£ Define Loss and Optimizer\n",
    "# ---------------------------------------------------\n",
    "# Mean Squared Error (MSE) loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Stochastic Gradient Descent optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4Ô∏è‚É£ Training Loop\n",
    "# ---------------------------------------------------\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # üß≠ Forward Pass ‚Äî Generate Predictions\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # üìâ Compute Loss\n",
    "    l = criterion(Y, y_pred)\n",
    "\n",
    "    # üßÆ Backward Pass ‚Äî Compute Gradients Automatically\n",
    "    l.backward()\n",
    "\n",
    "    # ‚öôÔ∏è Update Weights and Bias (handled by optimizer)\n",
    "    optimizer.step()\n",
    "\n",
    "    # üßπ Reset gradients before next iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # ü™Ñ Log progress every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f\"Epoch {epoch+1:03d}: w = {w[0][0].item():.3f}, b = {b.item():.3f}, loss = {l:.8f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5Ô∏è‚É£ After Training ‚Äî Evaluate Model\n",
    "# ---------------------------------------------------\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRU0PKH9K0dD73/OyTxD3G",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": "buildllm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
