{"cells":[{"cell_type":"markdown","id":"ae1226e0","metadata":{"id":"ae1226e0"},"source":["# LoRA Demo\n","\n","This demo aims to build a portable and efficient sentiment-analysis training pipeline using RoBERTa and LoRA.\n","\n","The code must run seamlessly in both Google Colab and local environments by automatically loading Hugging Face tokens from Colab userdata or a local .env file.\n","\n","The model requires preprocessing steps such as tokenization, adding a missing PAD token, and configuring label mappings. To reduce memory usage, the system uses parameter-efficient fine-tuning with LoRA. The goal is to train, evaluate, and compare the model‚Äôs performance before and after fine-tuning on the GLUE SST-2 dataset."]},{"cell_type":"code","source":["!pip install datasets>=4.4.1 transformers>=4.57.3 peft>=0.18.0 evaluate>=0.4.6 huggingface_hub python-dotenv>=1.0.1"],"metadata":{"id":"5Ffa3pf2_6w-","executionInfo":{"status":"ok","timestamp":1764425824195,"user_tz":360,"elapsed":4359,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"id":"5Ffa3pf2_6w-","execution_count":81,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Detect if running in Google Colab\n","def in_colab():\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False\n","\n","if in_colab():\n","    from google.colab import userdata\n","    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN_WRITE\")\n","else:\n","    # Running locally ‚Üí load from .env\n","    from dotenv import load_dotenv\n","    load_dotenv()  # loads variables from .env into environment\n","    os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n"],"metadata":{"id":"YWABw_OiROtu","executionInfo":{"status":"ok","timestamp":1764425824461,"user_tz":360,"elapsed":265,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"id":"YWABw_OiROtu","execution_count":82,"outputs":[]},{"cell_type":"code","execution_count":83,"id":"227f396f","metadata":{"id":"227f396f","executionInfo":{"status":"ok","timestamp":1764425824477,"user_tz":360,"elapsed":14,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["from datasets import load_dataset, Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer\n",")\n","from peft import LoraConfig, get_peft_model, TaskType\n","import torch\n","import evaluate\n","import numpy as np\n"]},{"cell_type":"code","source":["# -----------------------------\n","# Device & dtype setup\n","# -----------------------------\n","import torch\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n","\n","print(f\"Using device: {device} with dtype: {torch_dtype}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzb58EqUBcUs","executionInfo":{"status":"ok","timestamp":1764425824491,"user_tz":360,"elapsed":12,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"b015d0b3-f747-41e6-e9d9-44d51e33e753"},"id":"mzb58EqUBcUs","execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda with dtype: torch.float16\n"]}]},{"cell_type":"code","execution_count":85,"id":"849910cd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"849910cd","executionInfo":{"status":"ok","timestamp":1764425826064,"user_tz":360,"elapsed":1572,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"9b177cd5-0dc2-461f-f78a-c979935804ef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 67349\n","    })\n","    validation: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 872\n","    })\n","    test: Dataset({\n","        features: ['sentence', 'label', 'idx'],\n","        num_rows: 1821\n","    })\n","})"]},"metadata":{},"execution_count":85}],"source":["# Load dataset\n","# Standard GLUE SST-2 dataset - Sentiment Analysis of given sentences\n","dataset =  load_dataset(\"glue\", \"sst2\")\n","dataset"]},{"cell_type":"code","execution_count":86,"id":"f1859a58","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1859a58","executionInfo":{"status":"ok","timestamp":1764425828695,"user_tz":360,"elapsed":2632,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"75338b1f-63ce-4fce-f1c7-43ec607930a2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Before adding PAD token, tokenizer vocalbulary size: 50265\n","Before adding PAD token, tokenizer padding token: <pad>\n","After adding PAD token, tokenizer vocalbulary size: 50265\n","After adding PAD token, tokenizer padding token: <pad>\n"]}],"source":["base_model_name = \"FacebookAI/roberta-base\"\n","fine_tunned_model_name = \"roberta-sentiment-analysis\"\n","\n","# define label maps\n","id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n","label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n","\n","# AutoConfig loads RoBERTa‚Äôs default configuration but overrides some fields:\n","# num_labels=2 ‚Üí adds a classification head with 2 output labels\n","# id2label and label2id ‚Üí maps between label IDs and label names\n","config = AutoConfig.from_pretrained(\n","    base_model_name,\n","    num_labels=2,\n","    id2label=id2label,\n","    label2id=label2id,\n",")\n","\n","# Loads tokenizer for RoBERTa\n","# This tokenizer: Splits text into tokens, converts tokens to IDs, and handles special tokens like [CLS], [SEP], and [PAD].\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","# RoBERTa itself is not specifically a classification model; it's a general language model.\n","# But when we load it with AutoModelForSequenceClassification, it becomes a classifier.\n","# why? Because we specify the config with num_labels=2\n","# how? By using AutoModelForSequenceClassification, we are telling the model to add a classification head on top of the base RoBERTa model.\n","# You will see an output layer added \"(out_proj): Linear(in_features=768, out_features=3, bias=True)\"\n","base_model = AutoModelForSequenceClassification.from_pretrained(\n","    base_model_name,\n","    config=config,\n","    device_map=\"auto\" if device == \"cuda\" else None,\n","    torch_dtype=torch_dtype,\n",")\n","\n","base_model.to(device)\n","\n","print(f\"Before adding PAD token, tokenizer vocalbulary size: {len(tokenizer)}\")\n","print(f\"Before adding PAD token, tokenizer padding token: {tokenizer.pad_token}\")\n","\n","# Padding is needed because transformer models (like RoBERTa, BERT, GPT)\n","# only work with fixed-length batches, but sentences in real life have variable lengths.\n","# RoBERTa does NOT have a pad token by default.\n","# It uses the <mask> token as padding‚Äîbut this is not ideal for training.\n","if tokenizer.pad_token is None:\n","    print(\"Adding PAD token to tokenizer and resizing model embeddings...\")\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    base_model.resize_token_embeddings(len(tokenizer))\n","\n","print(f\"After adding PAD token, tokenizer vocalbulary size: {len(tokenizer)}\")\n","print(f\"After adding PAD token, tokenizer padding token: {tokenizer.pad_token}\")\n"]},{"cell_type":"code","execution_count":87,"id":"ee2899a9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee2899a9","executionInfo":{"status":"ok","timestamp":1764425828716,"user_tz":360,"elapsed":22,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"56b2fa35-0077-49c4-9381-7bde4ebec2f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]}],"source":["# Base Model Structure\n","print(base_model)"]},{"cell_type":"code","execution_count":88,"id":"94b85c50","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292,"referenced_widgets":["d400e79b238840d0a0322b9ff44a17bf","704d1f118aa74e5e9fb666698ae3fd2a","49151ad3151b4ad8bfc6193d3c346f88","939360cf48a6451d88fa153ea82c0397","b013d88961a84a78ae43758516b2160a","952d57236f6f4ad69fc1414c874318c9","4379dc34a9e04052bc1d921f03b11141","935d011e5b25458e9a60301c8b37e360","a640643b2f5d440893c9bdc24483da4c","6f1ed8c241404dd5b4d798f2d31fbbf9","4187f207d6054897a3ea1c26e3630e80"]},"id":"94b85c50","executionInfo":{"status":"ok","timestamp":1764425829523,"user_tz":360,"elapsed":806,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"3d1a3813-01bc-4cc5-d4e7-67de5de38251"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d400e79b238840d0a0322b9ff44a17bf"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n","        num_rows: 67349\n","    })\n","    validation: Dataset({\n","        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n","        num_rows: 872\n","    })\n","    test: Dataset({\n","        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n","        num_rows: 1821\n","    })\n","})"]},"metadata":{},"execution_count":88}],"source":["# Create tokenization function\n","# this function will be applied to each record in the dataset\n","# it extracts the sentence, tokenizes it to IDs, and truncates/pads to max length of 512\n","def tokenize_function(examples):\n","    # extract the sentence\n","    sentences = examples[\"sentence\"]\n","    # tokenize and truncate/pad to max length\n","    tokenizer.truncation_side = 'left'\n","    tokenized_inputs = tokenizer(\n","        sentences,\n","        return_tensors='np',\n","        truncation=True,\n","        max_length=512\n","    )\n","    return tokenized_inputs\n","\n","# tokenize training and validation datasets\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","tokenized_datasets\n"]},{"cell_type":"code","execution_count":89,"id":"2c0d03f4","metadata":{"id":"2c0d03f4","executionInfo":{"status":"ok","timestamp":1764425829570,"user_tz":360,"elapsed":43,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["# Data collator to dynamically pad the inputs received, so they are of equal length within a batch\n","# Data collators are used to batch multiple samples of data together and prepare it for training.\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":90,"id":"a326ab7c","metadata":{"id":"a326ab7c","executionInfo":{"status":"ok","timestamp":1764425830161,"user_tz":360,"elapsed":592,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["# evaluate library (by Hugging Face) lets you load standard evaluation metrics.\n","# it caldculates accuracy by comparing predicted labels to true labels.\n","accuracy_metric = evaluate.load(\"accuracy\")\n","\n","# define evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)}"]},{"cell_type":"code","execution_count":91,"id":"1e3daf4e","metadata":{"id":"1e3daf4e","executionInfo":{"status":"ok","timestamp":1764425830167,"user_tz":360,"elapsed":5,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["# Define a list of samples for testing the tokenizer\n","text_list = [\n","    # Positive\n","    \"I loved the new Batman movie!\",\n","    \"What an amazing experience!\",\n","    \"The service was surprisingly good, even though the restaurant was packed.\",\n","    \"Absolutely fantastic performance, though a bit too long for my taste.\",\n","    \"The concert had incredible energy, yet the sound quality was pleasing.\",\n","    \"The dessert was delightful.\",\n","    \"I loved the artwork.\",\n","    \"The new phone works well.\",\n","    \"The flight was smooth.\",\n","    \"I was thrilled by the surprise party.\",\n","\n","    # Negative\n","    \"The food at that restaurant was terrible.\",\n","    \"I will never go back to that place again.\",\n","    \"I was disappointed that my favorite dish was sold out.\",\n","    \"The book was thrilling at first, but the ending left me mindblowing.\",  # could be positive/negative, marking negative\n","    \"The hotel room looked nothing like the photos online, but the staff were friendly.\",  # neutral ‚Üí marking negative\n","    \"The movie had stunning visuals, but the plot was overly predictable.\",\n","    \"The customer support solved my issue quickly, though I had to wait on hold for a long time.\",\n","    \"I appreciated the thoughtful gift, but the packaging was damaged upon delivery.\"\n","]\n","\n"]},{"cell_type":"markdown","id":"ab7e9dac","metadata":{"id":"ab7e9dac"},"source":["## Test the BASE MODEL (before Finetunning)"]},{"cell_type":"code","execution_count":92,"id":"01141bbd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01141bbd","executionInfo":{"status":"ok","timestamp":1764425831059,"user_tz":360,"elapsed":893,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"6d614060-5142-4753-dab2-ebcfb7c11e20"},"outputs":[{"output_type":"stream","name":"stdout","text":["Untrained model predictions:\n","Text: I loved the new Batman movie! - POSITIVE\n","\n","Text: What an amazing experience! - POSITIVE\n","\n","Text: The service was surprisingly good, even though the restaurant was packed. - POSITIVE\n","\n","Text: Absolutely fantastic performance, though a bit too long for my taste. - POSITIVE\n","\n","Text: The concert had incredible energy, yet the sound quality was pleasing. - POSITIVE\n","\n","Text: The dessert was delightful. - POSITIVE\n","\n","Text: I loved the artwork. - POSITIVE\n","\n","Text: The new phone works well. - POSITIVE\n","\n","Text: The flight was smooth. - POSITIVE\n","\n","Text: I was thrilled by the surprise party. - POSITIVE\n","\n","Text: The food at that restaurant was terrible. - POSITIVE\n","\n","Text: I will never go back to that place again. - POSITIVE\n","\n","Text: I was disappointed that my favorite dish was sold out. - POSITIVE\n","\n","Text: The book was thrilling at first, but the ending left me mindblowing. - POSITIVE\n","\n","Text: The hotel room looked nothing like the photos online, but the staff were friendly. - POSITIVE\n","\n","Text: The movie had stunning visuals, but the plot was overly predictable. - POSITIVE\n","\n","Text: The customer support solved my issue quickly, though I had to wait on hold for a long time. - POSITIVE\n","\n","Text: I appreciated the thoughtful gift, but the packaging was damaged upon delivery. - POSITIVE\n","\n"]}],"source":["print(\"Untrained model predictions:\")\n","for text in text_list:\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","\n","    # Move inputs to same device as model\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    outputs = base_model(**inputs)\n","    predictions = torch.argmax(outputs.logits, dim=-1)\n","    print(f\"Text: {text} - {id2label[predictions.item()]}\")\n","    print()"]},{"cell_type":"markdown","id":"0ec30d64","metadata":{"id":"0ec30d64"},"source":["## Finetunning using LoRA"]},{"cell_type":"code","execution_count":93,"id":"656b6dfc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"656b6dfc","executionInfo":{"status":"ok","timestamp":1764425831082,"user_tz":360,"elapsed":4,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"8546868d-b0aa-4979-8ce8-d05b310c45b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Base model size: 124.65 million parameters\n"]}],"source":["# print base model size\n","base_model_size = sum(param.numel() for param in base_model.parameters())\n","print(f\"Base model size: {base_model_size/1e6:.2f} million parameters\")\n"]},{"cell_type":"code","execution_count":94,"id":"57be36dc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57be36dc","executionInfo":{"status":"ok","timestamp":1764425831288,"user_tz":360,"elapsed":205,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"a861591e-bb36-480c-e928-dbc4d6000379"},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 665,858 || all params: 125,313,028 || trainable%: 0.5314\n"]}],"source":["# Define LoRA configuration\n","lora_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS, # Task type = sequence classification (e.g., sentiment analysis). This tells LoRA which parts of the model to modify.\n","    inference_mode=False, # Set to False because you are training. True would freeze the base model for inference.\n","    r=4, # Rank of the low-rank decomposition. LoRA inserts small weight matrices of size r instead of modifying the full weight matrix.\n","    lora_alpha=32, # Scaling factor for LoRA weights (helps control magnitude).\n","    lora_dropout=0.1, # Dropout applied to LoRA layers during training (prevents overfitting).\n","    target_modules=[\"query\"] # target_modules specifies which parts of the transformer model will get LoRA adapters.\n","                            # In multi-head attention, each attention layer has weights for query (Q), key (K), value (V), and output (O)\n","                            # By setting target_modules=[\"query\"], LoRA will only inject trainable adapters into the query weight matrices.\n","                            # This reduces the number of trainable parameters even further.\n",")\n","\n","peft_model = get_peft_model(base_model, lora_config)\n","peft_model.print_trainable_parameters()\n"]},{"cell_type":"code","execution_count":95,"id":"0e332b24","metadata":{"id":"0e332b24","executionInfo":{"status":"ok","timestamp":1764425831289,"user_tz":360,"elapsed":1,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["# # peft_model size\n","# peft_model_size = sum(param.numel() for param in peft_model.parameters())\n","# print(f\"PEFT model size: {peft_model_size/1e6:.2f} million parameters\")\n"]},{"cell_type":"code","execution_count":96,"id":"51fd7ac3","metadata":{"id":"51fd7ac3","executionInfo":{"status":"ok","timestamp":1764425831392,"user_tz":360,"elapsed":102,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["# Hyperparameters\n","learning_rate = 2e-4\n","batch_size = 16\n","num_epochs = 3\n","weight_decay = 0.01\n","\n","# Training Configuration\n","training_args = TrainingArguments(\n","    output_dir=f\"./outputs/{fine_tunned_model_name}\",\n","    learning_rate=learning_rate,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    weight_decay=weight_decay,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=False,\n","    report_to=\"none\", #turns off WANDB reporting\n",")\n"]},{"cell_type":"code","execution_count":97,"id":"62192bb0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"62192bb0","executionInfo":{"status":"ok","timestamp":1764426334841,"user_tz":360,"elapsed":503447,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"bfea146e-779d-49e5-96c6-55fa46652360"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1111618688.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","The model is already on multiple devices. Skipping the move to device specified in `args`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 08:21, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.280200</td>\n","      <td>0.233765</td>\n","      <td>{'accuracy': 0.9174311926605505}</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.284700</td>\n","      <td>0.252930</td>\n","      <td>{'accuracy': 0.9128440366972477}</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.256600</td>\n","      <td>0.233032</td>\n","      <td>{'accuracy': 0.9277522935779816}</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=12630, training_loss=0.2815699766858268, metrics={'train_runtime': 501.3775, 'train_samples_per_second': 402.984, 'train_steps_per_second': 25.191, 'total_flos': 3759436993003656.0, 'train_loss': 0.2815699766858268, 'epoch': 3.0})"]},"metadata":{},"execution_count":97}],"source":["# Train the model using Trainer API\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","\n",")\n","# Train the model\n","trainer.train()\n"]},{"cell_type":"code","execution_count":106,"id":"931f3bc1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"931f3bc1","executionInfo":{"status":"ok","timestamp":1764426397447,"user_tz":360,"elapsed":1570,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"4c0ef08f-dcc8-474b-9f59-e77906f6283a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='110' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [55/55 01:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation results: {'eval_loss': 0.23291015625, 'eval_accuracy': {'accuracy': 0.9277522935779816}, 'eval_runtime': 1.2066, 'eval_samples_per_second': 722.7, 'eval_steps_per_second': 45.583, 'epoch': 3.0}\n","Fine-tuned model predictions:\n","Text: I loved the new Batman movie!\n","Text: I loved the new Batman movie! - POSITIVE\n","\n","Text: What an amazing experience!\n","Text: What an amazing experience! - POSITIVE\n","\n","Text: The service was surprisingly good, even though the restaurant was packed.\n","Text: The service was surprisingly good, even though the restaurant was packed. - POSITIVE\n","\n","Text: Absolutely fantastic performance, though a bit too long for my taste.\n","Text: Absolutely fantastic performance, though a bit too long for my taste. - POSITIVE\n","\n","Text: The concert had incredible energy, yet the sound quality was pleasing.\n","Text: The concert had incredible energy, yet the sound quality was pleasing. - POSITIVE\n","\n","Text: The dessert was delightful.\n","Text: The dessert was delightful. - POSITIVE\n","\n","Text: I loved the artwork.\n","Text: I loved the artwork. - POSITIVE\n","\n","Text: The new phone works well.\n","Text: The new phone works well. - POSITIVE\n","\n","Text: The flight was smooth.\n","Text: The flight was smooth. - POSITIVE\n","\n","Text: I was thrilled by the surprise party.\n","Text: I was thrilled by the surprise party. - POSITIVE\n","\n","Text: The food at that restaurant was terrible.\n","Text: The food at that restaurant was terrible. - NEGATIVE\n","\n","Text: I will never go back to that place again.\n","Text: I will never go back to that place again. - POSITIVE\n","\n","Text: I was disappointed that my favorite dish was sold out.\n","Text: I was disappointed that my favorite dish was sold out. - NEGATIVE\n","\n","Text: The book was thrilling at first, but the ending left me mindblowing.\n","Text: The book was thrilling at first, but the ending left me mindblowing. - POSITIVE\n","\n","Text: The hotel room looked nothing like the photos online, but the staff were friendly.\n","Text: The hotel room looked nothing like the photos online, but the staff were friendly. - POSITIVE\n","\n","Text: The movie had stunning visuals, but the plot was overly predictable.\n","Text: The movie had stunning visuals, but the plot was overly predictable. - NEGATIVE\n","\n","Text: The customer support solved my issue quickly, though I had to wait on hold for a long time.\n","Text: The customer support solved my issue quickly, though I had to wait on hold for a long time. - POSITIVE\n","\n","Text: I appreciated the thoughtful gift, but the packaging was damaged upon delivery.\n","Text: I appreciated the thoughtful gift, but the packaging was damaged upon delivery. - NEGATIVE\n","\n"]}],"source":["# Evaluate the model (evaluating peft model without merging to base)\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation results: {eval_results}\")\n","# Test the fine-tuned model\n","print(\"Fine-tuned model predictions:\")\n","for text in text_list:\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","\n","    # Move inputs to same device as model\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    outputs = peft_model(**inputs)\n","    predictions = torch.argmax(outputs.logits, dim=-1)\n","    print(f\"Text: {text}\")\n","    print(f\"Text: {text} - {id2label[predictions.item()]}\")\n","    print()"]},{"cell_type":"markdown","source":["## Upload LoRA adapter to hugging face"],"metadata":{"id":"zVPvrd_4CDOl"},"id":"zVPvrd_4CDOl"},{"cell_type":"code","source":["# -----------------------------\n","# Save & Push Only LoRA Adapter\n","# -----------------------------\n","adapter_name = \"roberta-lora-adapter\"\n","peft_model.save_pretrained(f\"./{adapter_name}\")\n","tokenizer.save_pretrained(f\"./{adapter_name}\")\n","peft_model.push_to_hub(f\"mishrabp/{adapter_name}\", use_auth_token=os.environ[\"HF_TOKEN\"])\n","tokenizer.push_to_hub(f\"mishrabp/{adapter_name}\", use_auth_token=os.environ[\"HF_TOKEN\"])\n","\n","# -----------------------------\n","# README for LoRA Adapter\n","# -----------------------------\n","readme_content = \"\"\"\n","---\n","license: mit\n","tags:\n","  - text-classification\n","  - sentiment-analysis\n","  - lora\n","  - peft\n","language: en\n","library_name: transformers\n","base_model: FacebookAI/roberta-base\n","datasets:\n","  - glue\n","---\n","\n","# üìò RoBERTa Sentiment Analysis ‚Äî LoRA Adapter Only\n","\n","This repository contains the **LoRA adapter** for fine-tuning **RoBERTa** on 2-class sentiment analysis (Negative, Positive) using GLUE SST-2 dataset.\n","The base model **FacebookAI/roberta-base** is NOT included, only the trainable LoRA adapters.\n","\n","## üöÄ How to Use\n","\n","```python\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from peft import PeftModel\n","import torch\n","\n","base_model_name = \"FacebookAI/roberta-base\"\n","adapter_model = \"mishrabp/roberta-lora-adapter\"\n","\n","# Load base model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n","\n","# Load LoRA adapter\n","model = PeftModel.from_pretrained(base_model, adapter_model)\n","\n","# Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","# Example inference\n","texts = [\"I loved the new Batman movie!\", \"The food was terrible.\"]\n","inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","predictions = torch.argmax(outputs.logits, dim=-1)\n","id2label = {0:\"NEGATIVE\",1:\"POSITIVE\"}\n","print([id2label[i.item()] for i in predictions])"],"metadata":{"id":"85dgmk8LCIBI"},"id":"85dgmk8LCIBI","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Merge Lora Adapter to the Base Model"],"metadata":{"id":"HE8ocwIN1Un7"},"id":"HE8ocwIN1Un7"},{"cell_type":"code","source":["# -------------------------------\n","# Step 1: Merge LoRA into base model\n","# -------------------------------\n","from peft import PeftModel\n","\n","# Merge and unload LoRA adapter ‚Üí returns a standard AutoModelForSequenceClassification\n","merged_model = peft_model.merge_and_unload()\n","\n","# -------------------------------\n","# Step 2: Save merged model locally\n","# -------------------------------\n","merged_model_name = \"roberta-sentiment-analysis-merged\"\n","merged_model.save_pretrained(merged_model_name)\n","tokenizer.save_pretrained(merged_model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EcWLQvWN1ZoJ","executionInfo":{"status":"ok","timestamp":1764426456914,"user_tz":360,"elapsed":21409,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"251fedf3-5742-4c1a-82b8-bc3a62d98220"},"id":"EcWLQvWN1ZoJ","execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('roberta-sentiment-analysis-merged/tokenizer_config.json',\n"," 'roberta-sentiment-analysis-merged/special_tokens_map.json',\n"," 'roberta-sentiment-analysis-merged/vocab.json',\n"," 'roberta-sentiment-analysis-merged/merges.txt',\n"," 'roberta-sentiment-analysis-merged/added_tokens.json',\n"," 'roberta-sentiment-analysis-merged/tokenizer.json')"]},"metadata":{},"execution_count":107}]},{"cell_type":"markdown","id":"a2a0fdf1","metadata":{"id":"a2a0fdf1"},"source":["## Upload the merge model to Hugging Face"]},{"cell_type":"code","source":["# -----------------------------\n","# Save & push adapters to Hugging Face\n","# -----------------------------\n","merged_model.save_pretrained(f\"./{merged_model_name}\")\n","tokenizer.save_pretrained(f\"./{merged_model_name}\")\n","\n","merged_model.push_to_hub(f\"mishrabp/{merged_model_name}\", use_auth_token=os.environ[\"HF_TOKEN\"])\n","tokenizer.push_to_hub(f\"mishrabp/{merged_model_name}\", use_auth_token=os.environ[\"HF_TOKEN\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254,"referenced_widgets":["ba13b257868740d19ca71102bf5765c7","b7495278d1e847678f77b4ae405a0d94","b89936be557141a69108505084859bf3","2a94a1d60f9748c9be331b0676c95107","f26c9dac3f434d6199536febf48fc44a","82f74be2543047478134fb347eb6b271","579c292ba5cb465aa29cac03b264fa00","f9daf48401724c2fa14c6eb8eacae40d","75e5018685db46e39d579c4e70013c2e","37c297ab352f46558a73f53e17f9ef12","b8540e14ac414648b246864de51f62e6","b5643aceb22e40ab9c86b22249967156","50a6abba4d8846a497fb1a52a4243ede","4e7a0e2b72b24dfd9e4c1672bb5dde45","457a99306cb34e6a8f65623c9b4331b6","c1656f99ba1547c698819439cbff16fa","1a3ced10dcf74774b64f416b4f1edfe1","d37be06d9f944ea2af52034988d6fdc2","bc3c48f5f91641158f46685e763d8ceb","f0cc45b9ea954b7ca163ea2f4c3d8679","4d64aac9ba924b939a9d150a4e860aae","9aa0055e42174c348f6c960b8b766e52","d8f4e88df8bd45ebab4690512e7eaaa7","cd8954d22eab41b9b94fc0ec0d4acc17","1eb1687e416747c3b15d895dca857a6f","fb499f3079c143bcbd441490394c79aa","6db63728a6e041bcb9c422a2c5a9f7c0","852ee04142e64556825d76f71c80f11c","dce3dbda6b6647799ad40fcdf158c47b","722c43683ee34be281afdf8e323722a8","5f86ad34c5004d8f887e6c9a59e67b39","d3b6f18d1a7f418db319cc5e794f354d","f5ab7e02628f46fea25d507f7cddb450"]},"id":"PUSVZABYtYPz","executionInfo":{"status":"ok","timestamp":1764426515659,"user_tz":360,"elapsed":52760,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"255d38cd-9c25-4255-fb9f-225aa5fab407"},"id":"PUSVZABYtYPz","execution_count":108,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:917: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba13b257868740d19ca71102bf5765c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["New Data Upload               : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5643aceb22e40ab9c86b22249967156"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  ...-merged/model.safetensors:  13%|#3        | 33.5MB /  249MB            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f4e88df8bd45ebab4690512e7eaaa7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"output_type":"execute_result","data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/mishrabp/roberta-sentiment-analysis-merged/commit/7e95506f68c9bc6d5ef0516c1e3375c7dbffcd57', commit_message='Upload tokenizer', commit_description='', oid='7e95506f68c9bc6d5ef0516c1e3375c7dbffcd57', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mishrabp/roberta-sentiment-analysis-merged', endpoint='https://huggingface.co', repo_type='model', repo_id='mishrabp/roberta-sentiment-analysis-merged'), pr_revision=None, pr_num=None)"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":108}]},{"cell_type":"code","execution_count":112,"id":"ec88db41","metadata":{"id":"ec88db41","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1764426624954,"user_tz":360,"elapsed":873,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"83052b94-8adc-41d2-db53-15591381c202"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/mishrabp/roberta-sentiment-analysis-merged/commit/f506d9baa314c07d4e88a802c7527d5b4ff5d21a', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='f506d9baa314c07d4e88a802c7527d5b4ff5d21a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mishrabp/roberta-sentiment-analysis-merged', endpoint='https://huggingface.co', repo_type='model', repo_id='mishrabp/roberta-sentiment-analysis-merged'), pr_revision=None, pr_num=None)"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":112}],"source":["import os\n","from google.colab import userdata\n","os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN_WRITE\")\n","\n","readme_content = \"\"\"\n","---\n","license: mit\n","tags:\n","  - text-classification\n","  - sentiment-analysis\n","  - sequence-classification\n","  - roberta\n","  - lora\n","  - peft\n","language: en\n","library_name: transformers\n","base_model: FacebookAI/roberta-base\n","datasets:\n","  - glue\n","---\n","\n","# üìò RoBERTa Sentiment Analysis ‚Äî LoRA Merged Model\n","\n","This repository contains a **LoRA fine-tuned RoBERTa model** for **2-class sentiment analysis** (Negative, Positive).\n","The base model is **`FacebookAI/roberta-base`**, and the LoRA adapters have been **merged into the base model** to produce a standalone model.\n","\n","---\n","\n","## üöÄ Model Overview\n","\n","| Feature | Details |\n","|--------|---------|\n","| **Base Model** | `FacebookAI/roberta-base` |\n","| **Fine-tuning method** | LoRA (PEFT) |\n","| **Task** | Sentiment Classification |\n","| **Labels** | NEGATIVE (0), POSITIVE (1) |\n","| **Dataset** | GLUE SST-2 |\n","| **Merged** | Yes, LoRA adapter merged into base model |\n","| **Training Environment** | Auto-detect (Google Colab or local machine) |\n","\n","---\n","\n","## üß† How to Use the Merged Model\n","\n","### üîπ Inference Example\n","\n","```python\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","\n","# -----------------------------\n","# Load merged model and tokenizer\n","# -----------------------------\n","model_name = \"mishrabp/roberta-sentiment-analysis-merged\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","# Make sure model is on the correct device and in evaluation mode\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","# -----------------------------\n","# Label mapping (same as training)\n","# -----------------------------\n","id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n","\n","# -----------------------------\n","# Texts for validation\n","# -----------------------------\n","text_list = [\n","    # Positive\n","    \"I loved the new Batman movie!\",\n","    \"What an amazing experience!\",\n","    \"The service was surprisingly good, even though the restaurant was packed.\",\n","    \"Absolutely fantastic performance, though a bit too long for my taste.\",\n","    \"The concert had incredible energy, yet the sound quality was pleasing.\",\n","    \"The dessert was delightful.\",\n","    \"I loved the artwork.\",\n","    \"The new phone works well.\",\n","    \"The flight was smooth.\",\n","    \"I was thrilled by the surprise party.\",\n","\n","    # Negative\n","    \"The food at that restaurant was terrible.\",\n","    \"I will never go back to that place again.\",\n","    \"I was disappointed that my favorite dish was sold out.\",\n","    \"The book was thrilling at first, but the ending left me mindblowing.\",  # could be positive/negative, marking negative\n","    \"The hotel room looked nothing like the photos online, but the staff were friendly.\",  # neutral ‚Üí marking negative\n","    \"The movie had stunning visuals, but the plot was overly predictable.\",\n","    \"The customer support solved my issue quickly, though I had to wait on hold for a long time.\",\n","    \"I appreciated the thoughtful gift, but the packaging was damaged upon delivery.\"\n","]\n","\n","# -----------------------------\n","# Inference\n","# -----------------------------\n","# Tokenize all texts as a batch (avoids inconsistencies and is faster)\n","inputs = tokenizer(\n","    text_list,\n","    return_tensors=\"pt\",\n","    truncation=True,\n","    padding=True,\n","    max_length=512\n",")\n","\n","# Move all inputs to device\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","# Run inference\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    predictions = torch.argmax(outputs.logits, dim=-1)\n","\n","# Print results\n","for text, pred in zip(text_list, predictions):\n","    print(f\"Text: {text}\")\n","    print(f\"Predicted Sentiment: {id2label[pred.item()]}\")\n","    print(\"-\" * 50)\n","```\n","\n","---\n","\n","## ‚öôÔ∏è Model Details\n","\n","- Merged LoRA adapters for efficient fine-tuning.\n","- Fully compatible with `AutoModelForSequenceClassification`.\n","- Trained on GLUE SST-2 dataset for 2-class sentiment analysis.\n","\n","---\n","\n","## üìù Citation\n","\n","```\n","@model{mishrabp_roberta_sentiment_analysis_merged,\n","  author = {Mishra, Bibhu},\n","  title = {RoBERTa Sentiment Analysis - LoRA Merged},\n","  year = 2025,\n","  base_model = {FacebookAI/roberta-base}\n","}\n","```\n","\n","---\n","\n","## üôå Acknowledgements\n","\n","- Hugging Face Transformers\n","- Hugging Face PEFT\n","- GLUE Benchmark\n","- RoBERTa by Facebook AI\n","\n","\n","\"\"\"\n","\n","with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(readme_content)\n","\n","from huggingface_hub import HfApi, Repository\n","\n","repo_id = f\"mishrabp/{merged_model_name}\"\n","\n","# Option 1: Using HfApi to upload README\n","api = HfApi()\n","api.upload_file(\n","    path_or_fileobj=\"README.md\",\n","    path_in_repo=\"README.md\",  # must be exactly README.md for HF Hub\n","    repo_id=repo_id,\n","    repo_type=\"model\",\n","    token=os.environ[\"HF_TOKEN\"]\n",")\n"]},{"cell_type":"markdown","source":["## Inference (from Hugging Face)"],"metadata":{"id":"-PrEeerLxCi2"},"id":"-PrEeerLxCi2"},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","\n","# -----------------------------\n","# Load merged model and tokenizer\n","# -----------------------------\n","model_name = \"mishrabp/roberta-sentiment-analysis-merged\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","# Make sure model is on the correct device and in evaluation mode\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","# -----------------------------\n","# Label mapping (same as training)\n","# -----------------------------\n","id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n","\n","# -----------------------------\n","# Texts for validation\n","# -----------------------------\n","text_list = [\n","    # Positive\n","    \"I loved the new Batman movie!\",\n","    \"What an amazing experience!\",\n","    \"The service was surprisingly good, even though the restaurant was packed.\",\n","    \"Absolutely fantastic performance, though a bit too long for my taste.\",\n","    \"The concert had incredible energy, yet the sound quality was pleasing.\",\n","    \"The dessert was delightful.\",\n","    \"I loved the artwork.\",\n","    \"The new phone works well.\",\n","    \"The flight was smooth.\",\n","    \"I was thrilled by the surprise party.\",\n","\n","    # Negative\n","    \"The food at that restaurant was terrible.\",\n","    \"I will never go back to that place again.\",\n","    \"I was disappointed that my favorite dish was sold out.\",\n","    \"The book was thrilling at first, but the ending left me mindblowing.\",  # could be positive/negative, marking negative\n","    \"The hotel room looked nothing like the photos online, but the staff were friendly.\",  # neutral ‚Üí marking negative\n","    \"The movie had stunning visuals, but the plot was overly predictable.\",\n","    \"The customer support solved my issue quickly, though I had to wait on hold for a long time.\",\n","    \"I appreciated the thoughtful gift, but the packaging was damaged upon delivery.\"\n","]\n","\n","# -----------------------------\n","# Inference\n","# -----------------------------\n","# Tokenize all texts as a batch (avoids inconsistencies and is faster)\n","inputs = tokenizer(\n","    text_list,\n","    return_tensors=\"pt\",\n","    truncation=True,\n","    padding=True,\n","    max_length=512\n",")\n","\n","# Move all inputs to device\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","# Run inference\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    predictions = torch.argmax(outputs.logits, dim=-1)\n","\n","# Print results\n","for text, pred in zip(text_list, predictions):\n","    print(f\"Text: {text}\")\n","    print(f\"Predicted Sentiment: {id2label[pred.item()]}\")\n","    print(\"-\" * 50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjU0khosxFrv","executionInfo":{"status":"ok","timestamp":1764426567337,"user_tz":360,"elapsed":4805,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"f425aae3-2f11-4427-fa24-06d5b2516ad3"},"id":"OjU0khosxFrv","execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: I loved the new Batman movie!\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: What an amazing experience!\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The service was surprisingly good, even though the restaurant was packed.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: Absolutely fantastic performance, though a bit too long for my taste.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The concert had incredible energy, yet the sound quality was pleasing.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The dessert was delightful.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: I loved the artwork.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The new phone works well.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The flight was smooth.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: I was thrilled by the surprise party.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The food at that restaurant was terrible.\n","Predicted Sentiment: NEGATIVE\n","--------------------------------------------------\n","Text: I will never go back to that place again.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: I was disappointed that my favorite dish was sold out.\n","Predicted Sentiment: NEGATIVE\n","--------------------------------------------------\n","Text: The book was thrilling at first, but the ending left me mindblowing.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The hotel room looked nothing like the photos online, but the staff were friendly.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: The movie had stunning visuals, but the plot was overly predictable.\n","Predicted Sentiment: NEGATIVE\n","--------------------------------------------------\n","Text: The customer support solved my issue quickly, though I had to wait on hold for a long time.\n","Predicted Sentiment: POSITIVE\n","--------------------------------------------------\n","Text: I appreciated the thoughtful gift, but the packaging was damaged upon delivery.\n","Predicted Sentiment: NEGATIVE\n","--------------------------------------------------\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3.12 (.venv)","language":"python","name":"buildllm-venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d400e79b238840d0a0322b9ff44a17bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_704d1f118aa74e5e9fb666698ae3fd2a","IPY_MODEL_49151ad3151b4ad8bfc6193d3c346f88","IPY_MODEL_939360cf48a6451d88fa153ea82c0397"],"layout":"IPY_MODEL_b013d88961a84a78ae43758516b2160a"}},"704d1f118aa74e5e9fb666698ae3fd2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_952d57236f6f4ad69fc1414c874318c9","placeholder":"‚Äã","style":"IPY_MODEL_4379dc34a9e04052bc1d921f03b11141","value":"Map:‚Äá100%"}},"49151ad3151b4ad8bfc6193d3c346f88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_935d011e5b25458e9a60301c8b37e360","max":1821,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a640643b2f5d440893c9bdc24483da4c","value":1821}},"939360cf48a6451d88fa153ea82c0397":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f1ed8c241404dd5b4d798f2d31fbbf9","placeholder":"‚Äã","style":"IPY_MODEL_4187f207d6054897a3ea1c26e3630e80","value":"‚Äá1821/1821‚Äá[00:00&lt;00:00,‚Äá3405.27‚Äáexamples/s]"}},"b013d88961a84a78ae43758516b2160a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"952d57236f6f4ad69fc1414c874318c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4379dc34a9e04052bc1d921f03b11141":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"935d011e5b25458e9a60301c8b37e360":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a640643b2f5d440893c9bdc24483da4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f1ed8c241404dd5b4d798f2d31fbbf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4187f207d6054897a3ea1c26e3630e80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba13b257868740d19ca71102bf5765c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7495278d1e847678f77b4ae405a0d94","IPY_MODEL_b89936be557141a69108505084859bf3","IPY_MODEL_2a94a1d60f9748c9be331b0676c95107"],"layout":"IPY_MODEL_f26c9dac3f434d6199536febf48fc44a"}},"b7495278d1e847678f77b4ae405a0d94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82f74be2543047478134fb347eb6b271","placeholder":"‚Äã","style":"IPY_MODEL_579c292ba5cb465aa29cac03b264fa00","value":"Processing‚ÄáFiles‚Äá(1‚Äá/‚Äá1)‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá:‚Äá100%"}},"b89936be557141a69108505084859bf3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9daf48401724c2fa14c6eb8eacae40d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75e5018685db46e39d579c4e70013c2e","value":1}},"2a94a1d60f9748c9be331b0676c95107":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c297ab352f46558a73f53e17f9ef12","placeholder":"‚Äã","style":"IPY_MODEL_b8540e14ac414648b246864de51f62e6","value":"‚Äá‚Äá249MB‚Äá/‚Äá‚Äá249MB,‚Äá‚Äá120MB/s‚Äá‚Äá"}},"f26c9dac3f434d6199536febf48fc44a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82f74be2543047478134fb347eb6b271":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"579c292ba5cb465aa29cac03b264fa00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9daf48401724c2fa14c6eb8eacae40d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"75e5018685db46e39d579c4e70013c2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37c297ab352f46558a73f53e17f9ef12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8540e14ac414648b246864de51f62e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5643aceb22e40ab9c86b22249967156":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50a6abba4d8846a497fb1a52a4243ede","IPY_MODEL_4e7a0e2b72b24dfd9e4c1672bb5dde45","IPY_MODEL_457a99306cb34e6a8f65623c9b4331b6"],"layout":"IPY_MODEL_c1656f99ba1547c698819439cbff16fa"}},"50a6abba4d8846a497fb1a52a4243ede":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a3ced10dcf74774b64f416b4f1edfe1","placeholder":"‚Äã","style":"IPY_MODEL_d37be06d9f944ea2af52034988d6fdc2","value":"New‚ÄáData‚ÄáUpload‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá:‚Äá"}},"4e7a0e2b72b24dfd9e4c1672bb5dde45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc3c48f5f91641158f46685e763d8ceb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0cc45b9ea954b7ca163ea2f4c3d8679","value":0}},"457a99306cb34e6a8f65623c9b4331b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d64aac9ba924b939a9d150a4e860aae","placeholder":"‚Äã","style":"IPY_MODEL_9aa0055e42174c348f6c960b8b766e52","value":"‚Äá‚Äá0.00B‚Äá/‚Äá‚Äá0.00B,‚Äá‚Äá0.00B/s‚Äá‚Äá"}},"c1656f99ba1547c698819439cbff16fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a3ced10dcf74774b64f416b4f1edfe1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d37be06d9f944ea2af52034988d6fdc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc3c48f5f91641158f46685e763d8ceb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f0cc45b9ea954b7ca163ea2f4c3d8679":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d64aac9ba924b939a9d150a4e860aae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aa0055e42174c348f6c960b8b766e52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8f4e88df8bd45ebab4690512e7eaaa7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd8954d22eab41b9b94fc0ec0d4acc17","IPY_MODEL_1eb1687e416747c3b15d895dca857a6f","IPY_MODEL_fb499f3079c143bcbd441490394c79aa"],"layout":"IPY_MODEL_6db63728a6e041bcb9c422a2c5a9f7c0"}},"cd8954d22eab41b9b94fc0ec0d4acc17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_852ee04142e64556825d76f71c80f11c","placeholder":"‚Äã","style":"IPY_MODEL_dce3dbda6b6647799ad40fcdf158c47b","value":"‚Äá‚Äá...-merged/model.safetensors:‚Äá100%"}},"1eb1687e416747c3b15d895dca857a6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_722c43683ee34be281afdf8e323722a8","max":249318428,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f86ad34c5004d8f887e6c9a59e67b39","value":249318428}},"fb499f3079c143bcbd441490394c79aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3b6f18d1a7f418db319cc5e794f354d","placeholder":"‚Äã","style":"IPY_MODEL_f5ab7e02628f46fea25d507f7cddb450","value":"‚Äá‚Äá249MB‚Äá/‚Äá‚Äá249MB‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá‚Äá"}},"6db63728a6e041bcb9c422a2c5a9f7c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"852ee04142e64556825d76f71c80f11c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dce3dbda6b6647799ad40fcdf158c47b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"722c43683ee34be281afdf8e323722a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f86ad34c5004d8f887e6c9a59e67b39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3b6f18d1a7f418db319cc5e794f354d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5ab7e02628f46fea25d507f7cddb450":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}