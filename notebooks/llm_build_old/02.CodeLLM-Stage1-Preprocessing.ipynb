{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUpXaW4AYqmA"
   },
   "source": [
    "# Data Preparation and Sampling\n",
    "\n",
    "![stage1](https://camo.githubusercontent.com/590a463dcb825375473c9fd366013e86204589d68be0bd0207d43b158ba10558/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30312e776562703f74696d657374616d703d31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16665,
     "status": "ok",
     "timestamp": 1761564724485,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "0T_3a44ycDSb",
    "outputId": "481c070f-38e6-4556-d8e2-4f4dfe24a4ec"
   },
   "outputs": [],
   "source": [
    "# Connect to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1761564803474,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "DCwTdT3SYONf",
    "outputId": "ea7b4841-9783-4010-df80-42b9ed3dda64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"../_data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJwZb-oZtxX1"
   },
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1761568316852,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "LPpt-C-npWPS",
    "outputId": "ad2c14fc-b21a-4b10-ba85-cbedd23a1adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "# Split the sentence to words (or tokens)\n",
    "# In the splitting process, remove space, commas, and periods.\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAj6fX-Yp7Ea"
   },
   "source": [
    "**Converting tokens into token IDs**\n",
    "![tokenids](https://camo.githubusercontent.com/bf01ba4b1b924633325cda845feac84ae1a3f154db5098f5d70e90470ff4484e/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30362e77656270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KSADFSjsuFH"
   },
   "source": [
    "### 2. Create Token Ids (Integers) + Build Vocabulary\n",
    "\n",
    "I am building a Vocabulary Dictionary and Sorting it Alphabetical Order. This is something that I would be using to give a numarical value to my tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761568580882,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "Tl1MVnjzp86n",
    "outputId": "71f3dc9f-ec81-4b64-910b-5c09c2bf31ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# From these tokens, we can now build a vocabulary that consists of all the unique tokens\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# Below are the first 50 entries in this vocabulary:\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s2d_DqNrAhA"
   },
   "source": [
    "Below, we illustrate the tokenization of a short sample text using a small vocabulary:\n",
    "![token vocab](https://camo.githubusercontent.com/8955d3aea45dc06f156d0579f7f3302c27b6635e649c301dbab33427b2d8d2a8/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30372e776562703f313233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnEAAHO0rUqn"
   },
   "source": [
    "**Putting it now all together into a tokenizer class**\n",
    "- Encoding the input text to Integers\n",
    "- Decoding the integers into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761568840208,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "UQRXC5z-rWEd"
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xHpsz9Jrn0f"
   },
   "source": [
    "- The encode function turns text into token IDs\n",
    "- The decode function turns token IDs back into text\n",
    "\n",
    "![tokenizer](https://camo.githubusercontent.com/b324e29fe9d3d4191a9200d6a08983eef4d3f835cff85ce1ee4aceb47117891a/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30382e776562703f313233)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1761569285589,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "ta-rvzPOsF8T",
    "outputId": "27b21594-e01f-4f61-d78f-07415089ea91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: \n",
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\n",
      "\n",
      "Encoded Tokens back to Text: \n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use the tokenizer to encode (that is, tokenize) texts into integers. These integers can then be embedded (later) as input of/for the LLM\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"Encoded Tokens: \")\n",
    "print(ids)\n",
    "print(\"\\n\")\n",
    "\n",
    "# We can decode the integers back into text\n",
    "print(\"Encoded Tokens back to Text: \")\n",
    "print(tokenizer.decode(ids))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRspYGn9uc4-"
   },
   "source": [
    "**Adding special context tokens**\n",
    "\n",
    "It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n",
    "\n",
    "![special tokens](https://camo.githubusercontent.com/11a0a59ffbb8eb8e6a90eb4ea7706e4be0d7ed9b53cadd0d31f676af267866c0/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30392e776562703f313233)\n",
    "\n",
    "#### Special Tokens in Tokenizers\n",
    "\n",
    "Some tokenizers use **special tokens** to help the LLM with additional context.\n",
    "\n",
    "##### Common Special Tokens\n",
    "\n",
    "- **[BOS]** (*Beginning of Sequence*) — Marks the beginning of text.  \n",
    "- **[EOS]** (*End of Sequence*) — Marks where the text ends.  \n",
    "  - This is usually used to concatenate multiple unrelated texts (e.g., two different Wikipedia articles or books).  \n",
    "- **[PAD]** (*Padding*) — Used when training LLMs with a batch size greater than 1.  \n",
    "  - Since texts may have different lengths, the padding token ensures that all sequences in a batch are of equal length by padding the shorter ones.  \n",
    "- **[UNK]** (*Unknown*) — Represents words that are not included in the vocabulary.  \n",
    "\n",
    "---\n",
    "\n",
    "##### GPT-2 and Special Tokens\n",
    "\n",
    "- **GPT-2** simplifies token usage — it **does not use** `[BOS]`, `[EOS]`, `[PAD]`, or `[UNK]`.  \n",
    "- Instead, GPT-2 uses a single token:  \n",
    "  - **`<|endoftext|>`** — analogous to the `[EOS]` token.  \n",
    "- GPT-2 also uses `<|endoftext|>` for padding since, during training, **attention masks** ensure that padded tokens are ignored.  \n",
    "- **No `<UNK>` token** — GPT-2 employs a **Byte Pair Encoding (BPE)** tokenizer, which breaks words into subword units instead of marking them as unknown.  \n",
    "\n",
    "---\n",
    "\n",
    "##### Usage Between Texts\n",
    "\n",
    "We use the **`<|endoftext|>`** token between two independent sources of text to separate them cleanly.\n",
    "\n",
    "![special context](https://camo.githubusercontent.com/1e57f90c79530539cb8e6c14233f52b88857f7f89afcd75fe0381fae7cf64ccd/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31302e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "error",
     "timestamp": 1761569841656,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "jWvsF0rvvV3V",
    "outputId": "f34b2755-721a-41b0-a61c-049092bc5f2a"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m tokenizer = SimpleTokenizerV1(vocab)\n\u001b[32m      5\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      9\u001b[39m preprocessed = [\n\u001b[32m     10\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     11\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# Let's what happen we encode a text which has words not present in the vocab we have built above.\n",
    "## The vocab we built above does not have the word \"Hello\". So, encoding a text with this word should raise an error.\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1761570253253,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "lQ-Se2GSvtuh",
    "outputId": "bbc478a0-3298-4884-d57f-d8784182ba0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above produces an error because the word \"Hello\" is not contained in the vocabulary\n",
    "# To deal with such cases, we can add special tokens like \"<|unk|>\" to the vocabulary to represent unknown words\n",
    "# Since we are already extending the vocabulary, let's add another token called \"<|endoftext|>\"\n",
    "# which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text,\n",
    "# like if our training datasets consists of multiple articles, books, etc.)\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1761570344655,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "oG7mI1NqwAau"
   },
   "outputs": [],
   "source": [
    "# Now let's update the tokenizer class SimpleTokenizerV2 to handle for unknown words\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761570347424,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "Uln9VT-UwcYL",
    "outputId": "d31d4e85-1c55-4200-9818-887e346f13d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea. Is this-- a test? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "\n",
      "Encoded: \n",
      "[1131, 5, 355, 1126, 628, 975, 7, 1131, 999, 6, 115, 1131, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "\n",
      "\n",
      "Decoded: \n",
      "<|unk|>, do you like tea. <|unk|> this -- a <|unk|>? <|endoftext|> In the sunlit terraces of the <|unk|>.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea. Is this-- a test?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Encoded: \")\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Decoded: \")\n",
    "print(tokenizer.decode(ids))\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MxUwe_r0jsd"
   },
   "source": [
    "### 3. BytePair encoding\n",
    "\n",
    "In Section-2 above, we learned about how Token-ID encoding is done. Here, we will learn the practical way using BPE.\n",
    "\n",
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer. (Splits words into subword units based on frequency.)\n",
    "\n",
    "- It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "\n",
    "- The original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "\n",
    "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source tiktoken library, which implements its core algorithms in Rust to improve computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1761571384900,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "LxXZ7xiY0nKl",
    "outputId": "294649b9-6e18-4749-a8a1-9a156c2a69e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken # OpenAI's tokenizer library\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1761571394916,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "MvoMGkwD1jY2"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761571423456,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "s_Niwf031l0C",
    "outputId": "a68f9c53-1440-4859-e26e-98f403ff8454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "\n",
      "\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)\n",
    "print(\"\\n\")\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB0PRsgj10Sl"
   },
   "source": [
    "BPE tokenizers break down unknown words into subwords and individual characters:\n",
    "\n",
    "![](https://camo.githubusercontent.com/5938dff392e5cb7404d2636e4d7157fceb4c36ecf57a2173001bd3edf22234da/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31312e77656270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fkNCuzK3cnJ"
   },
   "source": [
    "### 4. Sampling: Data sampling with a sliding window\n",
    "\n",
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:\n",
    "\n",
    "![datawithslidingwindow](https://camo.githubusercontent.com/b6245f4e6c64740c06f71ddd30d6495342b37315f0fd3556a0dc511be009a61f/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31322e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5475,
     "status": "ok",
     "timestamp": 1761572229932,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "N1kmMFX24vPc",
    "outputId": "faad8926-0f2a-4990-d2c8-9728e3ee0af8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IPMep9a47i0"
   },
   "source": [
    "We use a sliding window approach, changing the position by +1:\n",
    "![](https://camo.githubusercontent.com/9c738e75095f70d3dc4f6b3630008dd67607b5fa92e3bf776b0ed2cbb68db299/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31332e776562703f313233)\n",
    "\n",
    "Create dataset and dataloader that extract chunks from the input text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma13AVxJ_0LE"
   },
   "source": [
    "When training a model like GPT, you usually have a very long text, but the model can only process a limited number of tokens at a time (for example, 1024 or 2048). So we must split the long text into manageable chunks (tensors).\n",
    "\n",
    "That’s where max_length and stride come in.\n",
    "**max_length**: defines how many tokens go into each training example (tensor).\n",
    "\n",
    "For example, if:\n",
    " - max_length = 10\n",
    " - your text has 25 tokens → [t1, t2, …, t25]\n",
    "Then the dataset will split it like:\n",
    "```\n",
    "[ t1 … t10 ]\n",
    "[ t11 … t20 ]\n",
    "[ t21 … t25 ]\n",
    "\n",
    "```\n",
    "Each of these becomes one training example (input tensor).\n",
    "\n",
    "**stride**: Controls how much overlap there is between consecutive training chunks.\n",
    "\n",
    "If you use a stride smaller than max_length, you slide the window by fewer tokens, so the next chunk reuses some context from the previous one.\n",
    "\n",
    "```\n",
    "Suppose:\n",
    "max_length = 10\n",
    "stride = 5\n",
    "and your tokens are [t1, t2, t3, …, t25].\n",
    "\n",
    "Then your dataset chunks will be:\n",
    "Chunk 1: t1  to t10\n",
    "Chunk 2: t6  to t15\n",
    "Chunk 3: t11 to t20\n",
    "Chunk 4: t16 to t25\n",
    "```\n",
    "\n",
    "**Notice:**\n",
    "- The input text is converted into tokens (array of tokens)\n",
    "- Then, the complete token array is split into small arrays/chuncks/sampples (each array of max_length size). \n",
    "- stride is used to have an overlap of tokens between each consecutive chunks.\n",
    "\n",
    "**batch_size**: The number of samples (or tensors) processed in one batch (i.e one forward and backward pass of the model).\n",
    "\n",
    "**context window**: Is the maximum number of token a model can process. This is = max_length..\n",
    "E.g. GPT-3.5 token length is 16,384. It is the maximum number of token the model can process per batch.\n",
    "\n",
    "**stride**: stride controls how much the sliding window moves forward when chunking the tokenized text into overlapping sequences.\n",
    "\n",
    "Examples\n",
    "\n",
    "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "**With max_length=10 and stride=1** --> overlap of 9 (10-1) tokens\n",
    "\n",
    "Window 1: [ 0  1  2  3  4  5  6  7  8  9]\n",
    "Window 2: [ 1  2  3  4  5  6  7  8  9 10]\n",
    "Window 3: [ 2  3  4  5  6  7  8  9 10 11]\n",
    "Window 4: [ 3  4  5  6  7  8  9 10 11 12]\n",
    "...\n",
    "\n",
    "**With max_length=10 and stride=5**--> overlap of 5 (10-5) tokens\n",
    "\n",
    "Window 1: [ 0  1  2  3  4  5  6  7  8  9]\n",
    "Window 2: [ 5  6  7  8  9 10 11 12 13 14]\n",
    "Window 3: [10 11 12 13 14 15 16 17 18 19]\n",
    "\n",
    "**With max_length=10 and stride=10**--> overlap of 0 (10-0) tokens --> no overlap\n",
    "\n",
    "Window 1: [ 0  1  2  3  4  5  6  7  8  9]\n",
    "Window 2: [10 11 12 13 14 15 16 17 18 19]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **torch.utils.data.DataLoader** is one of PyTorch’s most powerful utilities — it’s not just a simple loop helper, it’s an engine that manages how your data flows into the model efficiently.\n",
    "\n",
    "The **DataLoader** wraps a Dataset and handles batching, shuffling, sampling, parallel loading, memory pinning, and collation.\n",
    "\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "print(\"\\n\")\n",
    "\n",
    "Understand the output here.\n",
    "\n",
    "[\n",
    "    tensor([[  40,  367, 2885, 1464],  --> this is the input tensor\n",
    "        [ 367, 2885, 1464, 1807]]),    --> the tensor has 2 array of size 4. This is because batch_size=2, max_length=4\n",
    "        \n",
    "    tensor([[ 367, 2885, 1464, 1807],  --> this is the target or exepected output tensor\n",
    "        [2885, 1464, 1807, 3619]])\n",
    "]\n",
    "\n",
    "**Note**\n",
    "\n",
    "- max_length is also called sequence length or context window of the model. It is nothing but the TOKEN SIZE of the model. \n",
    "- batch_size = number of sequences processed in parallel\n",
    "- embedding_dimension = is the dimension of the vector used by the model to store the tokens. It is internal to model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1761573624688,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "cAoo3-ox5ER1"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    ## max_length --> how many tokens will be placed in each tensor\n",
    "    ## stride --> ??\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,   # number of samples/chunks per batch\n",
    "        shuffle=shuffle,         # whether to shuffle the data at every epoch\n",
    "        drop_last=drop_last,     # true means that the last batch is dropped if it's smaller than batch_size\n",
    "        num_workers=num_workers  # >0 is used for parallel data loading\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1761574562394,
     "user": {
      "displayName": "Bibhu Mishra",
      "userId": "12646824449670614646"
     },
     "user_tz": 420
    },
    "id": "96neTBuQ6Aoz",
    "outputId": "b9b518d6-7478-4455-f95b-a66526bd1398"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Batch:\n",
      "Input Tensor: tensor([[  40,  367, 2885, 1464],\n",
      "        [ 367, 2885, 1464, 1807]])\n",
      "Decoded Tensor: I HAD always\n",
      "Decoded Tensor:  HAD always thought\n",
      "\n",
      "\n",
      "Expected Output Tensor: tensor([[ 367, 2885, 1464, 1807],\n",
      "        [2885, 1464, 1807, 3619]])\n",
      "Decoded Tensor:  HAD always thought\n",
      "Decoded Tensor: AD always thought Jack\n",
      "\n",
      "\n",
      "Second Batch:\n",
      "Input Tensor: tensor([[2885, 1464, 1807, 3619],\n",
      "        [1464, 1807, 3619,  402]])\n",
      "Decoded Tensor: AD always thought Jack\n",
      "Decoded Tensor:  always thought Jack G\n",
      "\n",
      "\n",
      "Expected Output Tensor: tensor([[1464, 1807, 3619,  402],\n",
      "        [1807, 3619,  402,  271]])\n",
      "Decoded Tensor:  always thought Jack G\n",
      "Decoded Tensor:  thought Jack Gis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=2\n",
    "max_length=4\n",
    "stride=1\n",
    "\n",
    "with open(\"../_data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=batch_size, max_length=max_length, stride=stride, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "print(\"First Batch:\")\n",
    "input1, target1 = next(data_iter)\n",
    "print(\"Input Tensor:\", input1)\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(input1[0].tolist()))\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(input1[1].tolist()))\n",
    "print(\"\\n\")\n",
    "print(\"Expected Output Tensor:\", target1)\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(target1[0].tolist()))\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(target1[1].tolist()))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Second Batch:\")\n",
    "input2, target2 = next(data_iter)\n",
    "print(\"Input Tensor:\", input2)\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(input2[0].tolist()))\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(input2[1].tolist()))\n",
    "print(\"\\n\")\n",
    "print(\"Expected Output Tensor:\", target2)\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(target2[0].tolist()))\n",
    "print(\"Decoded Tensor:\", tokenizer.decode(target2[1].tolist()))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Token Embeddings\n",
    "\n",
    "let us embed the tokens into a continuous vector representation using an embedding layer.\n",
    "Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training.\n",
    "\n",
    "![Embedding](https://camo.githubusercontent.com/cad0f97a119e344cc132e00a75f4b29baa23bae26eb9efa31fcb2ff44bc8b1cc/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31352e77656270)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n",
      "tensor([[ 0.4919,  0.5836, -0.4788, -0.5867, -0.4326,  0.0219, -0.7631, -0.3604,\n",
      "          0.3033,  0.7788,  0.5383, -0.5911,  1.1915,  2.0760,  0.4669,  0.2700,\n",
      "          0.7211,  1.2787, -1.0647,  0.2526,  1.0437,  0.7300, -0.3827,  0.3834,\n",
      "          0.1768, -0.8270, -1.2523, -0.8677,  1.4941, -0.7357, -0.1139, -1.9525,\n",
      "          0.8607, -0.0981,  0.0403, -0.6859,  1.7577, -0.3760,  0.4799,  1.3499,\n",
      "         -0.1287, -0.8746, -0.0395,  0.4360,  0.7254, -1.3802, -0.2777, -0.7300,\n",
      "          1.6153,  0.0424,  0.2142,  0.3498, -0.2399, -1.3444, -0.6124,  0.8006,\n",
      "          0.9300, -1.1502,  0.2665,  1.5674,  0.4683,  0.7776,  0.3345,  0.4141,\n",
      "         -0.0145, -1.5066, -1.3196,  0.8494, -0.9964,  0.5403,  0.3468,  1.5073,\n",
      "         -1.4258, -0.1474, -0.5213,  0.8206,  1.6428,  1.0459,  0.1935,  2.2498,\n",
      "         -0.9397,  0.9861,  0.4863, -0.5298, -0.7505,  0.9231, -0.3530, -0.1587,\n",
      "         -0.2162, -0.3538, -0.4384, -1.0969,  1.1176, -0.3549,  1.1794,  0.7936,\n",
      "         -0.4896,  1.1394,  0.3282,  0.6428, -0.0577,  0.2387, -0.2193, -0.9156,\n",
      "          0.0361,  1.4875, -0.2922, -0.6397,  0.8377, -1.6184, -0.4000,  0.4625,\n",
      "         -0.2467,  1.1690,  0.8485, -0.7803, -1.1214, -0.2170, -0.2066,  0.4545,\n",
      "         -1.4612, -0.2331, -0.8560,  0.5135,  1.5820,  0.9379,  0.5566, -0.5899],\n",
      "        [-0.5601,  0.3710, -0.5533,  0.1875, -0.3639, -2.4556, -1.4858,  0.5664,\n",
      "         -0.7344,  0.8815, -0.9062, -1.6737,  1.8377,  0.4989,  0.1015, -2.0812,\n",
      "          0.9306, -0.3158,  0.6506,  1.0635, -0.7188,  0.3271,  0.7218,  0.3755,\n",
      "          0.9224, -0.8367, -0.0770, -0.7434,  0.8730,  0.9767,  0.7236, -0.5617,\n",
      "         -0.9492,  0.4790,  0.9109, -0.1748, -0.7424,  1.3719, -0.5690,  0.7581,\n",
      "         -0.2163, -1.1611,  1.0867,  1.4387, -0.0568, -0.3121,  0.0424, -0.0658,\n",
      "         -1.9159,  1.0709,  1.0022, -0.6351,  0.4065,  0.2640,  0.9942,  0.6681,\n",
      "         -2.0319,  1.0053, -0.3339, -1.6982,  2.3924, -2.2281,  0.6459,  0.0617,\n",
      "          0.9660,  1.4964, -0.4921, -1.6472, -0.0264, -0.0154, -1.4194, -1.9373,\n",
      "          0.9203,  0.3528, -0.5358,  0.1050, -0.6478, -0.8532, -2.0537, -0.9254,\n",
      "          0.5700,  0.2709,  0.6253, -1.6848, -0.3661, -0.6758,  0.5928, -0.5072,\n",
      "         -0.2443,  1.4430, -0.2970, -0.6964,  0.3084,  0.1315, -1.5369,  0.4770,\n",
      "          1.1608,  0.5305,  0.9357,  0.9519,  0.4064, -1.0003, -0.4034, -1.2012,\n",
      "         -0.6087, -0.7078,  0.5237, -0.3076,  1.0332, -0.6395,  1.7075, -0.8561,\n",
      "          0.6569, -1.1815,  1.4140, -0.7771, -1.7236, -0.1737, -0.2367,  0.3926,\n",
      "         -0.8467,  0.4720,  0.5591,  0.6239, -0.0051, -1.1393,  1.0278, -1.8573],\n",
      "        [-0.6179, -0.7031,  0.2556, -0.7363,  1.2978,  0.0275, -0.7448, -0.8219,\n",
      "         -1.4409, -0.9123, -0.4860, -0.5355,  0.2971, -0.4001,  1.6594, -0.2805,\n",
      "          0.8951,  1.3696, -1.0544, -0.0099,  0.7910,  0.6239, -1.2379,  0.3528,\n",
      "          0.4941, -0.6988,  1.3184, -0.6615, -0.5676,  0.2921,  1.2872,  0.7475,\n",
      "         -1.2445,  0.5023,  0.0319,  0.3453, -1.3670, -0.0641,  0.6895, -0.1331,\n",
      "          0.7450,  0.6662, -1.0381,  1.6517,  0.7991, -0.9058, -1.6146,  0.7638,\n",
      "          1.2347,  1.3113, -0.7415,  0.9144, -1.4730, -1.2198,  0.2329, -0.4722,\n",
      "         -1.3820, -1.1840, -0.2645, -0.1461, -0.2768,  0.5989,  1.1743, -0.9740,\n",
      "          0.3816, -0.6641, -1.3623,  0.8028, -0.5627, -0.1722,  1.2403,  0.1355,\n",
      "          0.4561,  1.0377, -0.7983, -1.4851, -0.1879,  0.4193,  0.5844, -1.4239,\n",
      "         -1.2456,  0.8150,  0.1582,  1.1168, -0.1565,  0.9579, -0.8708,  1.3151,\n",
      "          1.5158, -0.0349, -0.6868, -0.4739, -0.3391, -0.0427,  0.1654, -0.0785,\n",
      "         -0.1953, -0.3399, -0.2178,  0.3289, -1.1250,  2.3737,  0.1087,  0.6434,\n",
      "          0.7183, -0.1428, -0.7276, -1.1425, -1.7549, -0.9629, -1.5556,  0.8566,\n",
      "          0.4393, -0.6080,  1.3149, -0.4629,  1.8647, -0.7754, -0.7536,  0.0955,\n",
      "          0.1422, -0.4454,  2.1803,  0.4996,  0.9798,  2.1906,  1.5081,  1.3302],\n",
      "        [ 0.8987,  0.6324, -0.1968, -0.6650, -0.0636, -1.2387,  1.7406,  0.7852,\n",
      "         -0.6962, -0.8977, -0.6769,  1.8434,  2.0989,  0.9230,  0.7512,  0.8682,\n",
      "         -1.6367,  1.3027, -0.7601, -0.3548,  1.5837, -0.0361, -0.1662,  2.1151,\n",
      "         -0.1725,  1.3778,  0.4736, -0.5516,  1.3316, -1.2898, -0.0485, -1.1055,\n",
      "          0.3414, -1.0349,  1.5347, -0.5278, -0.0639,  0.5755, -1.8004,  0.1347,\n",
      "         -0.2139,  0.3835,  1.1336, -0.8158, -0.4494, -0.7891, -1.1799,  1.6271,\n",
      "          0.3612,  0.3403,  1.2890,  0.8456,  0.2746,  0.7626, -0.4668, -0.6039,\n",
      "          0.0065,  1.9083, -0.1761,  0.1258,  1.1775, -0.1129,  1.1136,  0.8181,\n",
      "          0.2036, -2.6618,  0.7292,  1.3121, -0.3524, -0.7095, -1.0158, -0.2907,\n",
      "         -0.7351, -0.0581,  0.0436,  1.1241, -1.6490, -1.0699,  0.1436, -0.3393,\n",
      "          0.9230, -0.6029, -1.6078,  1.3677, -0.2892, -0.9053, -0.4020, -0.4570,\n",
      "          0.5065,  0.2841, -1.1128,  1.0511, -2.0541, -0.3929,  0.1630, -1.4645,\n",
      "         -0.9807, -0.2147,  1.0409,  0.6064,  0.4644, -0.2178,  1.1359, -0.4623,\n",
      "          0.6999, -0.4293,  0.1028,  0.1381, -1.2583, -0.8846,  0.3844,  0.9579,\n",
      "          1.6382,  0.8453,  0.0674, -0.0707, -1.3428,  0.4249, -0.5228,  0.0455,\n",
      "         -1.5956,  0.0028, -0.0704,  0.1675, -1.0572, -0.3730, -2.7053,  1.4351]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size,      # size of the vocabulary. That is, total number of unique tokens\n",
    "    embedding_dim=embedding_dim     # dimension of the embedding vector for each token\n",
    ")   \n",
    "\n",
    "# Example token IDs\n",
    "token_ids = torch.tensor([15496, 11, 703, 389])\n",
    "token_ids.shape\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = embedding_layer(token_ids)\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8572, -0.0949, -0.0674,  ...,  1.6262,  0.4949, -0.9278],\n",
      "         [-0.2643,  0.3225,  0.7887,  ...,  1.0567,  0.7091, -0.5430],\n",
      "         [ 0.0850, -0.9124, -0.6547,  ..., -1.0258, -0.3896, -1.1745],\n",
      "         [-1.3894,  0.3758,  0.1477,  ..., -0.7676, -1.3244, -0.3334]],\n",
      "\n",
      "        [[-0.2643,  0.3225,  0.7887,  ...,  1.0567,  0.7091, -0.5430],\n",
      "         [ 0.0850, -0.9124, -0.6547,  ..., -1.0258, -0.3896, -1.1745],\n",
      "         [-1.3894,  0.3758,  0.1477,  ..., -0.7676, -1.3244, -0.3334],\n",
      "         [ 0.5022,  1.2511, -0.2445,  ...,  0.2588, -0.2768,  0.2597]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 1.7934,  1.7447, -0.6078,  ..., -0.4965,  1.1797,  0.6800],\n",
      "        [-0.2222, -0.6934, -1.4048,  ...,  0.6152,  0.6212, -0.5946],\n",
      "        [-0.3884, -0.4185, -1.4137,  ..., -0.6162, -0.4294,  0.9796],\n",
      "        ...,\n",
      "        [-0.2919, -0.4982, -0.0549,  ..., -0.9170, -1.1971,  0.7040],\n",
      "        [-1.0929,  0.2177,  0.4344,  ...,  0.1323,  0.5005,  0.8721],\n",
      "        [-1.1093,  0.2520,  0.0222,  ...,  1.0289, -0.4581, -0.3355]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input1))\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Add Positional Encoding\n",
    "\n",
    "1. The Embedding layer above converts IDs into identical vector representations regardless of where they are located in the input sequence.\n",
    "\n",
    "![Embedding](https://camo.githubusercontent.com/2659e7bc3eed30da2e6a0e6adc3143d6240c2759e5315481b21877eb12de47e1/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31372e77656270)\n",
    "\n",
    "\n",
    "2. Positional embeddings are combined with the token embedding vector to form the FINAL input embeddings for a large language model processing.\n",
    "\n",
    "![Positional Encoding](https://camo.githubusercontent.com/e53fdceda6a07218acfe115d81dc930241569fbbcd5c3e533856dee1959a8a93/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31382e77656270)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 128])\n",
      "tensor([[[-1.3211,  0.6931,  0.7966,  ...,  1.7436,  1.3719, -0.4867],\n",
      "         [ 0.1186,  1.4148, -0.7753,  ...,  0.9467, -0.9491, -0.0198],\n",
      "         [ 0.7639, -0.6555, -0.5174,  ..., -0.3773,  0.6467, -2.9255],\n",
      "         [-0.5118,  2.2718, -0.8102,  ..., -1.5434, -0.1387, -1.1988]],\n",
      "\n",
      "        [[-2.4426,  1.1104,  1.6527,  ...,  1.1740,  1.5861, -0.1019],\n",
      "         [ 0.4679,  0.1800, -2.2187,  ..., -1.1358, -2.0478, -0.6512],\n",
      "         [-0.7105,  0.6326,  0.2850,  ..., -0.1191, -0.2880, -2.0844],\n",
      "         [ 1.3798,  3.1471, -1.2025,  ..., -0.5170,  0.9089, -0.6057]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-2.1783,  0.7880,  0.8640,  0.0499, -0.4130, -0.2857,  0.5581,  0.2588,\n",
      "         -0.8587,  1.8153, -0.5469, -1.2586,  0.4525, -0.0455,  0.9426, -0.7919,\n",
      "          1.0008,  0.5892, -0.2208, -0.6678,  1.6323, -0.8804,  0.0456,  0.6102,\n",
      "         -0.5970,  0.9962,  0.1780, -0.8567,  0.9069,  0.7035, -0.8602, -0.8870,\n",
      "         -0.1566,  0.7834,  1.6110,  1.0017, -0.2679, -0.8026, -0.8296, -0.8381,\n",
      "          0.1165,  0.8003, -0.1997, -0.3134,  0.6182, -0.1806,  0.3688,  1.4465,\n",
      "          1.9206,  0.8921, -0.5614, -0.4305, -0.1287, -1.6030,  1.5152, -0.2154,\n",
      "         -0.8587,  0.4140, -0.9479,  0.6986,  0.4587, -0.0656,  0.5082,  0.2136,\n",
      "         -1.1477,  1.6547,  0.5643, -0.3241,  0.0473, -0.6197, -0.4593, -0.5391,\n",
      "         -1.6794, -0.8387, -0.5740, -1.0936, -0.6734,  0.9154,  0.9182,  0.3842,\n",
      "          1.6519,  0.6483,  1.1268, -0.5071, -0.5190,  0.1765,  0.7394,  0.8724,\n",
      "          0.7599,  0.0225,  0.0185,  1.3238,  0.0108, -0.0449,  1.9544,  0.1433,\n",
      "         -0.6541,  0.1517,  0.2293, -0.2647,  1.7004,  0.3609,  0.4830,  0.2654,\n",
      "         -0.6323,  1.2707, -0.9372, -0.1557,  1.3510, -0.1382, -0.5263,  0.2905,\n",
      "          0.4200,  0.0967, -1.8796,  0.8891,  0.2765, -0.0604,  1.0194,  0.1143,\n",
      "         -1.0188, -1.9807, -0.7401, -1.3578,  0.8488,  0.1174,  0.8770,  0.4412],\n",
      "        [ 0.3829,  1.0923, -1.5640,  0.8198,  1.2594,  1.2383, -0.7642, -1.0001,\n",
      "          0.2248, -0.7910, -1.5088, -0.3643, -2.1620,  0.3757, -1.2393,  1.1106,\n",
      "         -0.0318, -1.4490,  1.2390, -0.5583,  1.4929,  0.4567,  0.4968, -1.7230,\n",
      "          0.1119, -1.8544, -0.1939, -1.3937,  0.5089, -1.6491, -0.5427, -0.3968,\n",
      "          0.4240, -0.0824,  0.6757, -0.6453,  0.8615, -2.4076, -0.6570,  0.6253,\n",
      "         -0.9798,  1.6766,  0.0571,  0.2487, -2.4024, -0.1313, -1.0702,  0.1629,\n",
      "          0.1303,  0.8216,  1.6065, -0.9897,  0.2720, -1.9410,  2.6037,  0.2578,\n",
      "          0.6463, -0.0724, -0.6905, -2.3294,  1.8687,  1.5607,  0.2753,  0.7614,\n",
      "          0.8913,  0.8420, -1.1901,  1.2091,  0.2055, -0.7387,  1.2450, -0.2998,\n",
      "         -0.2642,  1.4891, -0.0687, -0.5397,  0.2642, -1.3198, -1.6683, -1.8773,\n",
      "         -1.6445,  2.0519,  1.0948,  0.6413,  0.5398, -1.1643, -0.8318,  1.1634,\n",
      "          0.7823, -0.4524,  1.7984,  1.8868,  0.5865, -0.3326,  0.4031, -0.8094,\n",
      "          0.1159,  2.0038,  0.9331,  1.7645,  0.2614, -1.5185,  3.0310,  0.1382,\n",
      "          0.6922,  1.3945, -3.1503, -0.0385, -0.0637, -0.5703,  0.7420,  0.1841,\n",
      "          1.9594,  0.6124,  0.3058, -1.0550,  0.0788, -1.0370, -1.4968,  1.3096,\n",
      "          0.4769, -0.2760, -0.6975, -0.9067, -0.2295, -0.1100, -1.6582,  0.5233],\n",
      "        [ 0.6789,  0.2568,  0.1373, -1.8456, -0.9718, -1.6176, -0.2391,  0.4746,\n",
      "          1.1368, -0.1000, -1.1459,  0.8876,  0.9218,  0.4767,  0.6385, -1.2832,\n",
      "          0.0541, -0.9713,  0.6156,  0.1391, -1.7174,  1.0472, -1.9752, -0.9181,\n",
      "         -2.5163,  0.7726,  0.8025,  1.1104, -0.3358,  0.9970, -0.7490,  2.2008,\n",
      "         -0.7925,  0.8592, -0.7596, -1.4532,  1.3878,  0.5393, -1.5842,  0.6369,\n",
      "          0.5200,  0.6472, -1.1724, -0.1642,  0.0719, -2.2092,  0.4955, -1.0294,\n",
      "          1.2310, -0.2429,  0.0641, -0.7871,  0.1682,  0.6724,  2.0397, -0.3597,\n",
      "         -0.4126, -1.2837, -0.5448,  0.9860,  1.4976,  2.7604,  1.4593, -0.3771,\n",
      "          2.0032,  1.4184,  0.6486, -1.0228,  0.9195, -0.6908,  0.2231,  0.2101,\n",
      "          0.8370, -1.6002, -1.6105, -1.0936,  0.6629, -1.1361, -0.1804, -0.7792,\n",
      "         -1.2434, -0.6961, -0.1871, -0.7675, -0.3263, -0.5325,  0.4714, -2.2417,\n",
      "         -0.3077, -0.3616, -0.0306,  1.3753,  1.1604, -1.8096, -0.2259,  0.5867,\n",
      "         -0.7166,  1.8022, -0.2524,  0.2084,  0.0181, -1.3881, -2.0916,  1.9288,\n",
      "         -0.4487, -0.2527,  0.7886,  0.2821, -0.2220,  1.1166,  0.4509, -0.6601,\n",
      "         -1.5527,  0.1897, -1.1520, -0.3700,  0.8739,  0.1376, -1.1098,  1.0179,\n",
      "         -0.2584,  1.3742, -0.7096,  1.2960,  0.9610,  0.6485,  1.0364, -1.7511],\n",
      "        [ 0.8776,  1.8960, -0.9580, -0.8163,  0.8216, -0.1298, -0.0580,  0.3088,\n",
      "          1.2437,  1.1727,  0.2603, -0.6357, -0.6356, -0.1390,  0.4519,  0.4708,\n",
      "          0.2417,  1.0729,  1.1688, -1.0715,  0.0038, -0.3842,  0.9274, -0.9114,\n",
      "          0.0043, -0.0695, -0.0748, -0.0067, -0.0844,  0.2281, -0.1385, -1.1278,\n",
      "          0.4002, -1.3644, -0.4097, -2.0808, -0.1123,  1.0367,  0.0258,  0.7141,\n",
      "         -0.2293, -0.3217,  0.5989, -0.4279,  0.5835, -0.2382, -1.4021,  1.7558,\n",
      "         -1.9261, -0.4577, -0.1617, -0.5532, -0.2808,  0.1337, -1.3534,  0.8491,\n",
      "          0.8628, -0.1887,  1.0610,  1.2343, -0.1779,  1.2025, -0.1764, -0.1771,\n",
      "          1.3437,  0.1527, -0.3347,  2.3201, -0.5927,  0.7045,  0.0149,  1.2811,\n",
      "         -0.1450,  0.2640, -0.0360,  0.6972,  0.1840,  1.2439, -0.7247,  0.6729,\n",
      "          0.1478,  1.8113, -0.5606,  0.0170, -0.7219, -0.9216,  0.6600, -0.5748,\n",
      "         -1.2635, -0.3703, -0.8141, -0.4092, -0.9972,  1.5376, -0.7049,  1.0999,\n",
      "         -0.4794,  1.4243,  0.2347, -0.6199,  2.4712, -1.0297,  0.1248, -1.3757,\n",
      "         -1.0597,  0.7340, -0.2153,  0.6888, -0.1325, -0.4137, -0.6164,  0.3388,\n",
      "          0.4880, -0.9183,  2.2965, -0.0056,  0.9126,  0.7785,  0.6843,  0.2196,\n",
      "          0.9779, -0.5535,  0.4548,  2.0679,  0.4997, -0.7758,  1.1857, -0.8655]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding\n",
    "# Positional encoding is a key concept in transformer-based models (like GPT, BERT, etc.) \n",
    "# because, unlike RNNs, transformers don’t have a built-in notion of word order.\n",
    "# GPT-2 uses absolute position embeddings, so we just create another embedding layer:\n",
    "\n",
    "vocab_size = max_length  # maximum position index\n",
    "embedding_dim = 128\n",
    "positional_embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size,      # size of the vocabulary. That is, total number of unique tokens\n",
    "    embedding_dim=embedding_dim     # dimension of the embedding vector for each token\n",
    ")   \n",
    "\n",
    "pos_embeddings = positional_embedding_layer(torch.arange(max_length))\n",
    "\n",
    "# Add the positional encoding to the token embeddings\n",
    "input_embeddings = embedding_layer(input1) + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "print(input_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flow](https://camo.githubusercontent.com/730badacd85e476130cab5a98990d3c616b4333921096c576c31a50e7c0ca627/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31392e77656270)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP+xgAc+BaYvRxBUlkHCSkt",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
