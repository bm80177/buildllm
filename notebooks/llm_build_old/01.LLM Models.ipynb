{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP36ePQ4yT96hh2EBdokblt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer Detailing\n","https://colab.research.google.com/drive/1SLkv5mFi9Bns_PWmaliHmh080_C1GQyt#scrollTo=wu04l-sDZmJ4"],"metadata":{"id":"PqHgUWJtVEen"}},{"cell_type":"markdown","source":["# GPT Architecture\n","- GPT is built referring the Transformer architecture, but it there is no Encoder in it.\n","- GPT is a **Decoder only Transformer**.\n","- GPT is trained using unsupervised learning.(**next word in the sentence is used as the label**)\n","- **GPT is auto-regressive model**. It uses previous outputs as inputs for future predictions. E.g. It predicts the next word.\n","- GPT-3 has 96 Transformer layers, and 175 billions parameters.\n","- It was originally trained to predict next word. But It suprised the researcher with its **emergant behaviours** like translation, classification, content creation, etc.... It was never trained on such tasks.\n","\n","**Reference**:\n","https://www.youtube.com/watch?v=xbaYCf2FHSY&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=5\n"],"metadata":{"id":"Whjom5D_RO0-"}},{"cell_type":"markdown","source":["# Stages of Building an LLM from Scratch\n","\n","**Stage1:**\n","- Data Preparation and Sampling (Tokenization + Embedding + Positional Encoding)\n","- Attention Mechanism\n","- LLM Architecture (stacking the layers)\n","- Output: Input Data + Model Architecture\n","\n","**Stage2: (pretraining)**\n","- Training Loop\n","- Model Evaluation\n","- Load Pretrained Weights\n","- Output: Foundation Model\n","\n","**Stage3: (finetunning)**\n","- Classifier\n","- Personal Assistant"],"metadata":{"id":"NyNDt-T8VOZj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqA-lh28RLje"},"outputs":[],"source":[]}]}