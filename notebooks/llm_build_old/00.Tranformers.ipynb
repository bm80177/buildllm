{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu04l-sDZmJ4"
   },
   "source": [
    "# Transformers\n",
    "\n",
    "## ðŸ”Ž The Problem with RNNs and LSTMs\n",
    "- **Sequential bottleneck**: RNNs/LSTMs read one token at a time â†’ slow, hard to parallelize.  \n",
    "- **Long-term dependencies**: Struggle to capture relationships between distant tokens (even with LSTMsâ€™ gates).\n",
    "\n",
    "**Transformers (2017, _Attention is All You Need_)** solve both the issues. **Transformer** is a deep learning neural network architecture introduced in 2017 paper.\n",
    "\n",
    "---\n",
    "\n",
    "Models built using Transformers are for:\n",
    "- voice-to-text (takes audio and generates transcript)\n",
    "- text-to-voice (transcript to audio)\n",
    "- text-to-image (DALL-E, Mid-Journey)\n",
    "- machine translation\n",
    "- summerization\n",
    "\n",
    "**Reference**: https://poloclub.github.io/transformer-explainer/\n",
    "\n",
    "https://prvnsmpth.github.io/animated-transformer/\n",
    "\n",
    "## Transformer Architecture - Simplified\n",
    "Original transformer architecture was developed for machine translation task translating enligsh text to german and french.\n",
    "\n",
    "![transformer](https://media.geeksforgeeks.org/wp-content/uploads/20240729154418/Transformer.png)\n",
    "\n",
    "Before a Transformer (encoder, decoder, or encoderâ€“decoder) can process text, the input must be converted from **raw text** into **numerical matrices**.  \n",
    "This happens in three main stages: **Tokenization**, **Embedding**, and **Positional Encoding**.\n",
    "\n",
    "We'll use the same example throughout:\n",
    "\n",
    "> **Example Sentence:**  \n",
    "> \"Harry Potter was a highly unusual boy ... least favorite teacher, professor ??\"\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Transformer Input Processing (Tokenization â†’ Embedding â†’ Positional Encoding)\n",
    "\n",
    "### ðŸ”¹ Step 1: Tokenization\n",
    "\n",
    "The first step is to convert human-readable text into smaller, machine-readable chunks called **tokens**.\n",
    "\n",
    "Transformers use **subword tokenization** (e.g., **Byte Pair Encoding â€“ BPE**).  \n",
    "Tokens can be entire words, subwords, or even punctuation and whitespace.\n",
    "\n",
    "| Stage | Representation | Description |\n",
    "|--------|----------------|--------------|\n",
    "| Input Text | \"Harry Potter was a highly unusual boy ... least favorite teacher, professor ??\" | Original sentence |\n",
    "| Tokens | [\"Harry\", \" Potter\", \" was\", \" a\", \" highly\", \" unusual\", \" boy\", \"...\", \" least\", \" favorite\", \" teacher\", \",\", \" professor\", \" ??\"] | Subword tokens |\n",
    "| Token IDs | [11112, 23450, 312, 234, 1222, 5431, 2390, 891, 7654, 9876, 2343, 11, 4567, 98] | Integer IDs from vocabulary |\n",
    "\n",
    "**Key facts:**\n",
    "- GPT-3â€™s vocabulary dictionary has **50,257 unique tokens**.  \n",
    "- Each token ID is just an **index** into a large embedding table.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 2: Embedding\n",
    "\n",
    "Each token ID is converted into a **high-dimensional vector** through an **embedding lookup**.  \n",
    "For example, GPT-3 175B uses an **embedding dimension of 12,288**. Each vector is represented as matrix of [1 x 12288]\n",
    "\n",
    "For our 14-token input:\n",
    "- Each token (e.g., `\"Harry\"`) becomes a **12,288-dimensional vector**.\n",
    "- The entire sequence becomes a **matrix** with **14 rows** (one per token) and **12,288 columns** (features per token).\n",
    "\n",
    "| Token | Token ID | Embedding Vector Shape |\n",
    "|--------|-----------|------------------------|\n",
    "| \"Harry\" | 11112 | [1 Ã— 12,288] |\n",
    "| \"Potter\" | 23450 | [1 Ã— 12,288] |\n",
    "| ... | ... | ... |\n",
    "| \"??\" | 98 | [1 Ã— 12,288] |\n",
    "\n",
    "So, the final embedding matrix is of [14 tokens x 12288] dimensions.\n",
    "\n",
    "### ðŸ”¹ Step 3: Positional Encoding\n",
    "Transformers have **no inherent notion of word order**.  \n",
    "To fix this, a **positional encoding vector** is added to each tokenâ€™s embedding to indicate its **position in the sequence**.\n",
    "\n",
    "Formally:\n",
    "\n",
    "\\[\n",
    "Z = E + P\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- \\( E \\): embedding matrix `[sequence_length Ã— embedding_dim]`  \n",
    "- \\( P \\): positional encoding matrix `[sequence_length Ã— embedding_dim]`  \n",
    "- \\( Z \\): final input matrix passed into the Transformer layers\n",
    "\n",
    "For our example:\n",
    "\n",
    "| Token | Position | Embedding (E) | Positional Vector (P) | Combined (Z = E + P) |\n",
    "|--------|-----------|----------------|------------------------|-----------------------|\n",
    "| \"Harry\" | 0 | [1 Ã— 12,288] | [1 Ã— 12,288] | [1 Ã— 12,288] |\n",
    "| \"Potter\" | 1 | [1 Ã— 12,288] | [1 Ã— 12,288] | [1 Ã— 12,288] |\n",
    "| ... | ... | ... | ... | ... |\n",
    "\n",
    "Final matrix shape:\n",
    "[14 Ã— 12,288]\n",
    "\n",
    "\n",
    "If this input is batched with others, the model may **pad** all sequences to the same length (e.g., GPT-3â€™s context window of **2048 tokens**): [2048 Ã— 12,288]\n",
    "\n",
    "GPT-3 model can take 2048 tokens or words in one go.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Step 4: Feeding into the Transformer\n",
    "\n",
    "After combining embeddings and positional encodings, the resulting matrix \\( Z \\) enters the Transformer stack.  \n",
    "Hereâ€™s how it differs depending on the architecture:\n",
    "\n",
    "| Model Type | Next Step | Flow |\n",
    "|-------------|------------|------|\n",
    "| **Encoder-only** (e.g., BERT) | \\( Z \\) â†’ Encoder layers (self-attention) | Used for classification or understanding tasks |\n",
    "| **Decoder-only** (e.g., GPT-3) | \\( Z \\) â†’ Decoder layers (masked self-attention) | Used for text generation |\n",
    "| **Encoderâ€“Decoder** (e.g., T5, original Transformer) | Encoder processes input â†’ Decoder attends to encoder output | Used for translation, summarization, etc. |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Transformer Encoder Layers (Self-Attention)\n",
    "\n",
    "After tokenization, embedding, and positional encoding, our input is now a matrix of shape:\n",
    "\n",
    "```\n",
    "[sequence_length Ã— hidden_size]\n",
    "```\n",
    "\n",
    "For our example (**14 tokens** and **12,288 dimensions**):\n",
    "\n",
    "```\n",
    "Input Matrix: [14 Ã— 12,288]\n",
    "```\n",
    "\n",
    "This matrix passes through multiple **encoder layers** (12 for BERT Base, 24 for BERT Large, etc.).\n",
    "Each encoder layer has two key sublayers:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "2. **Feed-Forward Network (FFN)**\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Self-Attention â€” â€œWhat should I pay attention to?â€\n",
    "\n",
    "Each encoder layer allows every token to **interact with all other tokens** in the same sequence.\n",
    "\n",
    "#### Step 1: Linear Projections\n",
    "\n",
    "For each token vector, the model computes three new representations using learned weight matrices:\n",
    "\n",
    "* **Query (Q):** What am I looking for?\n",
    "* **Key (K):** What information do I have?\n",
    "* **Value (V):** The actual information to share\n",
    "\n",
    "Each has a shape of `[sequence_length Ã— d_k]`, e.g.:\n",
    "\n",
    "```\n",
    "[14 Ã— 64] per attention head\n",
    "```\n",
    "\n",
    "#### Step 2: Scaled Dot-Product Attention\n",
    "\n",
    "Attention scores are calculated as:\n",
    "\n",
    "$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$\n",
    "\n",
    "This mechanism helps each token â€œattendâ€ to others based on relevance.\n",
    "For example, **â€œHarryâ€** might attend strongly to **â€œPotterâ€**.\n",
    "\n",
    "#### Step 3: Multi-Head Attention\n",
    "\n",
    "Instead of one attention process, the model uses **multiple heads** (e.g., 96 in GPT-3).\n",
    "Each head learns different relationships (like subjectâ€“verb or entityâ€“reference).\n",
    "All head outputs are concatenated and linearly projected back into a single matrix:\n",
    "\n",
    "```\n",
    "[14 Ã— 12,288]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Add & Normalize\n",
    "\n",
    "The attention output is added back to the original input (**residual connection**) and normalized:\n",
    "\n",
    "$\n",
    "\\text{Output}_1 = \\text{LayerNorm}(X + \\text{SelfAttention}(X))\n",
    "$\n",
    "\n",
    "This stabilizes training and preserves the original information.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Feed-Forward Network (FFN)\n",
    "\n",
    "Each token vector is passed through a small two-layer neural network:\n",
    "\n",
    "$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$\n",
    "\n",
    "This expands the dimensionality (e.g., 12,288 â†’ 49,152 â†’ 12,288) to introduce non-linear transformations.\n",
    "\n",
    "A second **Add & Normalize** follows:\n",
    "\n",
    "$\n",
    "\\text{Output}_2 = \\text{LayerNorm}(\\text{Output}_1 + \\text{FFN}(\\text{Output}_1))\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ Final Encoder Output\n",
    "\n",
    "After stacking multiple encoder layers, we get the **final contextual representation** for each token.\n",
    "\n",
    "* **Shape remains:** `[14 Ã— 12,288]`\n",
    "* **But now:** each token vector is **context-aware** â€” e.g., â€œHarryâ€ knows it relates to â€œPotter,â€ not â€œteacher.â€\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Summary Table\n",
    "\n",
    "| Stage            | Input Shape   | Operation                | Output Shape  | Description                  |\n",
    "| ---------------- | ------------- | ------------------------ | ------------- | ---------------------------- |\n",
    "| Input Embeddings | [14 Ã— 12,288] | â€”                        | [14 Ã— 12,288] | Token + positional encoding  |\n",
    "| Self-Attention   | [14 Ã— 12,288] | Multi-head attention     | [14 Ã— 12,288] | Tokens attend to one another |\n",
    "| Add & Norm       | [14 Ã— 12,288] | Residual + normalization | [14 Ã— 12,288] | Stabilize learning           |\n",
    "| Feed-Forward     | [14 Ã— 12,288] | Linear â†’ ReLU â†’ Linear   | [14 Ã— 12,288] | Token-wise transformation    |\n",
    "| Add & Norm       | [14 Ã— 12,288] | Residual + normalization | [14 Ã— 12,288] | Final encoder output         |\n",
    "\n",
    "\n",
    "## ðŸ§© Transformer Decoder Layers (with Encoder Input for Translation)\n",
    "\n",
    "Assume we are performing a **translation task**, and the decoder receives **encoder outputs** from the input sentence:\n",
    "\n",
    "> Original Sentence: \"Harry Potter was a highly unusual boy ... least favorite teacher, professor ??\"\n",
    "\n",
    "The **encoder** has already processed this input and produced a **contextualized representation** for each token:\n",
    "\n",
    "```\n",
    "Encoder Output Matrix: [14 Ã— 12,288]\n",
    "```\n",
    "\n",
    "This matrix will serve as the **key and value** for the decoder's cross-attention layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Decoder Input\n",
    "\n",
    "The decoder takes its own target sequence as input, e.g., a partially generated translation:\n",
    "\n",
    "```\n",
    "Decoder Input Tokens: \"Harry Potter Ã©tait un garÃ§on ... professeur prÃ©fÃ©rÃ© ??\"\n",
    "Token IDs â†’ Embeddings â†’ Positional Encoding\n",
    "Decoder Input Matrix: [14 Ã— 12,288]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Masked Self-Attention\n",
    "\n",
    "* The decoder computes **queries, keys, and values** from its own input.\n",
    "* **Masking** ensures that token *i* only attends to previous tokens `0` to `i` (not future tokens).\n",
    "* This produces **context-aware decoder vectors** that respect autoregressive generation.\n",
    "\n",
    "```\n",
    "Shape: [14 Ã— 12,288]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Cross-Attention (Attending to Encoder Outputs)\n",
    "\n",
    "* The decoder's **queries (Q)** come from the masked self-attention output.\n",
    "* The **keys (K)** and **values (V)** come from the **encoder output** `[14 Ã— 12,288]`.\n",
    "\n",
    "[\n",
    "ext{CrossAttention}(Q_{dec}, K_{enc}, V_{enc}) = \text{softmax}\\left(\frac{Q_{dec} K_{enc}^T}{\\sqrt{d_k}}\n",
    "ight) V_{enc}\n",
    "]\n",
    "\n",
    "* This step allows each decoder token to **attend to relevant parts of the original input sentence**, e.g., â€œHarryâ€ in French aligns with â€œHarryâ€ in English.\n",
    "* Output shape: `[14 Ã— 12,288]`\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ Feed-Forward Network (FFN)\n",
    "\n",
    "* Each token vector is independently passed through a **2-layer feed-forward network** with residual connections and layer normalization.\n",
    "* Output shape remains `[14 Ã— 12,288]`.\n",
    "\n",
    "[\n",
    "ext{FFN}(x) = \text{ReLU}(xW_1 + b_1) W_2 + b_2\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ Final Decoder Output\n",
    "\n",
    "* After multiple decoder layers (e.g., 12â€“96), each token vector encodes:\n",
    "\n",
    "  * Its own past tokens (**masked self-attention**)\n",
    "  * Relevant information from the **encoder output** (**cross-attention**)\n",
    "\n",
    "* These vectors are then projected through a **linear + softmax layer** to predict the **next token probabilities** in the target language.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Summary Table (Decoder with Encoder Input)\n",
    "\n",
    "| Stage                            | Input Shape                                       | Operation                | Output Shape  | Description                                     |\n",
    "| -------------------------------- | ------------------------------------------------- | ------------------------ | ------------- | ----------------------------------------------- |\n",
    "| Embeddings + Positional Encoding | [14 Ã— 12,288]                                     | â€”                        | [14 Ã— 12,288] | Decoder input tokens                            |\n",
    "| Masked Self-Attention            | [14 Ã— 12,288]                                     | Multi-head attention     | [14 Ã— 12,288] | Attend to previous tokens only                  |\n",
    "| Add & Norm                       | [14 Ã— 12,288]                                     | Residual + normalization | [14 Ã— 12,288] | Stabilize learning                              |\n",
    "| Cross-Attention                  | [14 Ã— 12,288] (decoder) + [14 Ã— 12,288] (encoder) | Multi-head attention     | [14 Ã— 12,288] | Attend to encoder output (source sentence)      |\n",
    "| Add & Norm                       | [14 Ã— 12,288]                                     | Residual + normalization | [14 Ã— 12,288] | Stabilize learning                              |\n",
    "| Feed-Forward                     | [14 Ã— 12,288]                                     | Linear â†’ ReLU â†’ Linear   | [14 Ã— 12,288] | Token-wise transformation                       |\n",
    "| Add & Norm                       | [14 Ã— 12,288]                                     | Residual + normalization | [14 Ã— 12,288] | Final decoder output ready for token prediction |\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**\n",
    "> For translation tasks, the decoder layers combine **masked self-attention** and **cross-attention** to ensure that each generated token is informed by:\n",
    ">\n",
    "> 1. Its previous tokens in the target sequence\n",
    "> 2. The contextual representation of the input sequence from the encoder\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spy3ZALroare"
   },
   "source": [
    "# How GPT-3 Works: The Magic Behind the Next Word\n",
    "\n",
    "Have you ever wondered what's happening under the hood when a tool like GPT-3 or ChatGPT instantly generates a coherent and human-like response? The process, while complex, can be broken down into a series of logical steps. At its heart, GPT-3 is an incredibly powerful prediction machine, designed to do one thing exceptionally well: predict the next word in a sequence. This is the magic behind every poem, email, or line of code it writes.\n",
    "\n",
    "Hereâ€™s a step-by-step breakdown of how a model like GPT-3 predicts the missing word \"Snape\" in the sentence \"Harry Potter was a highly unusual boy ... least favorite teacher, professor ??\"\n",
    "\n",
    "## Step 1: Tokenization (BPE)\n",
    "The very first step is to translate the human-readable text into a format the model can understand. This process is called tokenization. GPT-3 uses Byte-Pair Encoding (BPE), which breaks down text into smaller units called tokens. These aren't just whole words; they can be parts of words or common symbols, making the model highly efficient.\n",
    "\n",
    "**Input Sentence:** \"Harry Potter was a highly unusual boy ... least favorite teacher, professor ??\"\n",
    "\n",
    "**Output Tokens:** [\"Harry\", \" Potter\", \" was\", \" a\", \" highly\", \" unusual\", \" boy\", \"...\", \" least\", \" favorite\", \" teacher\", \",\", \" professor\", \" ??\"]\n",
    "\n",
    "**Output IDs:** [11112, 23450, 312, 234, 1222, 5431, 2390, 891, 7654, 9876, 2343, 11, 4567, 98]\n",
    "\n",
    "Each token is then mapped to a unique integer ID from the model's large vocabulary of 50,257 tokens. This transforms the text into a sequence of numbers, which is the actual input for the next step.\n",
    "\n",
    "![transformer](https://media.geeksforgeeks.org/wp-content/uploads/20240729154418/Transformer.png)\n",
    "\n",
    "## Step 2: Embedding + Positional Encoding\n",
    "Next, these token IDs are converted into a more meaningful representation: a high-dimensional vector. This process is called embedding. Each token ID is looked up in a massive table, which maps it to a vector of 12,288 dimensions. This vector captures the token's semantic meaning.\n",
    "\n",
    "To tell the model about the order of the words, a positional encoding vector is added to each token's embedding. This unique vector tells the model the exact position of each token in the sequence (e.g., \"Harry\" is at position 0, \"Potter\" at position 1, and so on). Together, the embedding and positional encoding create a single, powerful vector that encodes both the token's meaning and its position.\n",
    "\n",
    "The model's context window is the maximum number of tokens it can process at once (e.g., 2048 tokens for some versions of GPT-3). For computational efficiency, all input sequences are padded to this fixed length.\n",
    "\n",
    "**Input IDs:** [11112, 23450, 312, ...]\n",
    "\n",
    "**Output Matrix:** A matrix where each row is a token's vector. The shape would be [14 tokens Ã— 12,288 dimensions], which is then padded to [2048 Ã— 12,288] for batch processing.\n",
    "\n",
    "## Step 3: The Transformer Decoder Blocks (Ã—96)\n",
    "The core of GPT-3 is a stack of 96 Transformer decoder blocks. Each block refines the token vectors by applying two key mechanisms that allow the model to understand the relationships between words.\n",
    "\n",
    "### A. Masked Multi-Head Self-Attention\n",
    "This is where the model determines which words are most relevant to each other to establish context. For example, when processing the token \"professor,\" the model's query for context finds a high similarity with the keys of \"teacher,\" \"favorite,\" and \"Harry Potter.\" This tells the model what information is most important for making its prediction.\n",
    "\n",
    "The multi-head part means the model does this not once, but in parallel across multiple \"heads,\" each of which learns to focus on a different aspect of the sentence (e.g., one head might focus on grammatical relationships, while another focuses on character names). The masked part ensures the model cannot \"cheat\" by looking at the future token it's trying to predict.\n",
    "\n",
    "#### Here's how it works at a matrix level:\n",
    "\n",
    "- **Creating the Query, Key, and Value Matrices:** The input is a matrix of token embeddings, X, with a shape of [T Ã— d_model]. For each attention head, the model learns three separate weight matrices (W_Q, W_K, and W_V) to project X into three new matrices: Q (Query), K (Key), and V (Value). The dimensions of these new matrices are d_k and d_v, which are typically smaller than d_model.\n",
    "\n",
    "- **Query Matrix (Q):** Represents what each token is \"looking for.\" Q = XW_Q. Output shape: [T Ã— d_k]\n",
    "\n",
    "- **Key Matrix (K):** Represents what each token \"contains\" to be found. K = XW_K. Output shape: [T Ã— d_k]\n",
    "\n",
    "- **Value Matrix (V):** Contains the actual content information for each token. V = XW_V. Output shape: [T Ã— d_v]\n",
    "\n",
    "- **Calculating Attention Scores:** The dot product of the Query and Key matrices is calculated to find the similarity between every pair of tokens. This creates a matrix of attention scores.\n",
    "\n",
    "**Attention Scores Matrix:** AttentionScores = QK^T. Output shape: [T Ã— T].\n",
    "\n",
    "This matrix is the heart of the attention mechanism. Each element (i, j) represents how much token i should \"pay attention\" to token j. In a decoder, a mask is applied to this matrix to prevent tokens from looking at future tokens.\n",
    "\n",
    "The scores are then scaled by the square root of d_k and a softmax function is applied, normalizing the scores into a probability distribution.\n",
    "\n",
    "- **Generating the Output Matrix:** The normalized attention weights are then multiplied by the Value matrix V. This creates a weighted sum of the values, where the weights are the attention scores.\n",
    "\n",
    "### B. Feed-Forward Network (FFN)\n",
    "After attention, each token's vector is processed by a two-layer neural network. This network adds abstract features and non-linearities, helping the model learn more complex patterns, such as recognizing that the current context is likely a slot for a character's name.\n",
    "\n",
    "**Input Matrix:** [14 Ã— 12,288] (from Step 2)\n",
    "\n",
    "**Output Matrix:** A new [14 Ã— 12,288] matrix where each vector is context-aware.\n",
    "\n",
    "- Row 1: [0.34, 0.67, 0.12, ...] (contextualized vector for \"Harry\")\n",
    "- Row 2: [0.88, 0.45, 0.91, ...] (contextualized vector for \"Potter\")\n",
    "- ...\n",
    "- Row 14: [0.91, 0.56, 0.23, ...] (contextualized vector for \"??\")\n",
    "\n",
    "**Output Shape:** [14 Ã— 12,288]\n",
    "\n",
    "## Step 4: Final Linear Layer + Softmax\n",
    "The final step is to use the deeply contextualized vector of the last token, \"??\", to make a prediction. This vector is passed through a final linear layer that projects it back to the size of the vocabulary (50,257). The output of this layer is a list of scores, called logits.\n",
    "\n",
    "A Softmax function is then applied to these logits, converting them into a probability distribution where the probabilities of all possible next tokens sum to 1. Based on the context \"Harry Potter... least favorite teacher, professor,\" the model would assign the highest probabilities to the most likely candidates:\n",
    "\n",
    "**Input Vector:** [1 Ã— 12,288] (the last row from the output of Step 3)\n",
    "\n",
    "**Logits Vector:** [1 Ã— 50,257]\n",
    "\n",
    "[..., 0.1, 0.5, 0.9, 0.05, ...] (before Softmax)\n",
    "\n",
    "**Softmax Output (Probabilities):**\n",
    "- P(\" Snape\"): 0.72\n",
    "- P(\" Dumbledore\"): 0.09\n",
    "- P(\" McGonagall\"): 0.05\n",
    "\n",
    "**Output Shape:** [1 Ã— 50,257]\n",
    "\n",
    "## Step 5: Autoregressive Generation\n",
    "Finally, the model selects the token with the highest probability (\"Snape\"), adds it to the sequence, and feeds this new, longer sequence back into the model. This process repeats, predicting one word at a time, until a stop token (like a period) is generated or a length limit is reached. This is how GPT-3 creates a complete and continuous output, one word at a time.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# How GPT-3 Reaches 175 Billion Parameters\n",
    "\n",
    "GPT-3â€™s massive size (175B parameters) comes from stacking many Transformer decoder blocks and the associated embedding layers. Hereâ€™s how those parameters add up:\n",
    "\n",
    "## 1. Embedding Layer\n",
    "- **Vocabulary size:** 50,257 tokens  \n",
    "- **Embedding dimension (`d_model`):** 12,288  \n",
    "- **Parameters:** 50,257 Ã— 12,288 â‰ˆ **617M**  \n",
    "\n",
    "This is the word embedding table (also reused for the output layer due to weight tying).\n",
    "\n",
    "## 2. Transformer Decoder Blocks (Ã—96)\n",
    "Each block has self-attention + feed-forward + layer norms.\n",
    "\n",
    "### A. Multi-Head Self-Attention\n",
    "- Query, Key, Value projections: 3 Ã— (12,288 Ã— 12,288) â‰ˆ 453M  \n",
    "- Output projection: 12,288 Ã— 12,288 â‰ˆ 151M  \n",
    "- **Total per attention:** â‰ˆ 604M  \n",
    "\n",
    "### B. Feed-Forward Network (FFN)\n",
    "- Expands hidden size 4Ã—: 12,288 â†’ 49,152 â†’ 12,288  \n",
    "- First linear: 12,288 Ã— 49,152 â‰ˆ 603M  \n",
    "- Second linear: 49,152 Ã— 12,288 â‰ˆ 603M  \n",
    "- **Total per FFN:** â‰ˆ 1.2B  \n",
    "\n",
    "### C. Layer Norms and Biases\n",
    "Small compared to others (millions).\n",
    "\n",
    "**Total per block:** â‰ˆ 1.8B parameters  \n",
    "**Across 96 blocks:** 96 Ã— 1.8B â‰ˆ **173B**\n",
    "\n",
    "## 3. Final Layer Norm + Output Head\n",
    "- Small, but output head ties with embeddings (~617M).\n",
    "\n",
    "## âœ… Grand Total\n",
    "- Embeddings: ~0.6B  \n",
    "- Transformer blocks: ~173B  \n",
    "- Misc + tied weights: ~1B  \n",
    "- **â‰ˆ 175B parameters**\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ **Key insight:** Most parameters (~99%) are in the **feed-forward networks**, not embeddings or attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhKzdYyv-c2Q"
   },
   "source": [
    "# How BERT Works: The Magic Behind Understanding Language\n",
    "\n",
    "Have you ever wondered how a model like BERT can understand the meaning\n",
    "of a sentence, answer questions, or detect sentiment? Unlike GPT, which\n",
    "is designed to *predict the next word*, BERT is built to deeply\n",
    "understand the **entire sentence in context**. This makes it especially\n",
    "powerful for tasks like search engines, question answering, and\n",
    "sentiment analysis.\n",
    "\n",
    "At its heart, BERT is not trying to write the next word---it's trying to\n",
    "**grasp meaning** by looking at all the words at once. Let's break down\n",
    "how BERT works with an example.\n",
    "\n",
    "Suppose we want BERT to figure out the missing word in this sentence:\n",
    "\n",
    "*\"Harry Potter was a highly unusual boy. His least favorite teacher was\n",
    "Professor \\[MASK\\].\"*\n",
    "\n",
    "We want BERT to predict the masked word (\"Snape\") based on the entire\n",
    "context.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Step 1: Tokenization (WordPiece)\n",
    "\n",
    "Just like GPT, BERT starts by breaking the text into smaller pieces\n",
    "called tokens. BERT uses **WordPiece tokenization**, which breaks rare\n",
    "words into subwords.\n",
    "\n",
    "**Input Sentence:**\\\n",
    "\"Harry Potter was a highly unusual boy. His least favorite teacher was\n",
    "Professor \\[MASK\\].\"\n",
    "\n",
    "**Output Tokens:**\\\n",
    "\\[\"\\[CLS\\]\", \"Harry\", \"Potter\", \"was\", \"a\", \"highly\", \"unusual\", \"boy\",\n",
    "\".\", \"His\", \"least\", \"favorite\", \"teacher\", \"was\", \"Professor\",\n",
    "\"\\[MASK\\]\", \".\", \"\\[SEP\\]\"\\]\n",
    "\n",
    "-   `[CLS]` is a special classification token added at the start.\\\n",
    "-   `[SEP]` marks the end of a sentence.\\\n",
    "-   `[MASK]` is a placeholder telling BERT to guess this word.\n",
    "\n",
    "Each token is then mapped to an ID, just like GPT.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Step 2: Embedding + Positional Encoding\n",
    "\n",
    "Every token ID is converted into a high-dimensional vector. But BERT\n",
    "embeddings are richer than GPT's---they include three kinds of\n",
    "information:\n",
    "\n",
    "1.  **Token Embedding:** The meaning of the token (e.g., \"Harry\").\\\n",
    "2.  **Segment Embedding:** Whether the token belongs to sentence A or B\n",
    "    (for tasks like Question + Answer).\\\n",
    "3.  **Position Embedding:** The token's position in the sequence (so\n",
    "    BERT knows word order).\n",
    "\n",
    "These vectors are summed up into one embedding for each token, forming\n",
    "the input matrix:\n",
    "\n",
    "**Input IDs â†’ Embedding Matrix:** Shape = \\[T tokens Ã— d_model\\]\\\n",
    "For BERT-Base, `d_model = 768`; for BERT-Large, `d_model = 1024`.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Step 3: Transformer Encoder Blocks (Ã—12 or Ã—24)\n",
    "\n",
    "Here's where BERT differs most from GPT. Instead of a **decoder-only\n",
    "stack** like GPT, BERT uses **encoder-only Transformer blocks**.\n",
    "\n",
    "### A. Bidirectional Self-Attention\n",
    "\n",
    "BERT's attention mechanism looks at *all* words at once, both left and\n",
    "right.\n",
    "\n",
    "-   When guessing `[MASK] = \"Snape\"`, BERT doesn't just look left\n",
    "    (\"Professor\") or right (\".\"), it also considers context like \"least\n",
    "    favorite teacher\" earlier in the sentence.\\\n",
    "-   This bidirectional nature allows it to truly understand context,\n",
    "    unlike GPT, which can only look backward.\n",
    "\n",
    "Technically, it works the same way as GPT's attention:\\\n",
    "- Each token generates a Query (Q), Key (K), and Value (V).\\\n",
    "- Attention scores = QK\\^T, normalized by softmax.\\\n",
    "- Each token becomes a **weighted combination of all other tokens**.\n",
    "\n",
    "But unlike GPT, there's **no masking**---BERT is allowed to see the full\n",
    "sentence.\n",
    "\n",
    "### B. Feed-Forward Networks\n",
    "\n",
    "After attention, each token's vector goes through a small neural network\n",
    "that enriches its representation.\n",
    "\n",
    "Output: Contextualized embeddings, where each token now \"knows\" its\n",
    "meaning relative to the full sentence.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Step 4: The Output Head (Masked Language Modeling)\n",
    "\n",
    "Now comes the special trick: BERT was trained with the **Masked Language\n",
    "Model (MLM)** objective.\n",
    "\n",
    "-   Some tokens (like \"Snape\") are replaced with `[MASK]`.\\\n",
    "-   BERT must predict the original word using all surrounding context.\n",
    "\n",
    "For our example:\n",
    "\n",
    "**Input Vector (for \\[MASK\\]):** \\[1 Ã— 768\\]\\\n",
    "**Logits Vector:** \\[1 Ã— 30,000\\] (BERT's vocab size)\n",
    "\n",
    "After applying **Softmax**, BERT assigns probabilities:\n",
    "\n",
    "-   P(\"Snape\") = 0.83\\\n",
    "-   P(\"Dumbledore\") = 0.06\\\n",
    "-   P(\"McGonagall\") = 0.04\n",
    "\n",
    "BERT picks **\"Snape\"**, which makes sense in context.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## Step 5: Fine-Tuning for Specific Tasks\n",
    "\n",
    "Unlike GPT, which is mostly used for text generation, BERT is designed\n",
    "for **fine-tuning**. After pretraining on billions of words, BERT can be\n",
    "adapted for:\n",
    "\n",
    "-   **Sentiment Analysis:** Feed in a sentence â†’ use `[CLS]` embedding â†’\n",
    "    classify as Positive/Negative.\\\n",
    "-   **Question Answering:** Feed in a question + passage â†’ BERT\n",
    "    highlights the answer span.\\\n",
    "-   **Named Entity Recognition:** Classify each token (e.g., \"Harry\" â†’\n",
    "    PERSON).\n",
    "\n",
    "This versatility is why BERT became the foundation for so many modern\n",
    "NLP models.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "# How BERT Reaches 340 Million Parameters\n",
    "\n",
    "BERT comes in two main sizes:\n",
    "\n",
    "-   **BERT-Base:** 110M parameters (12 layers, 12 heads, hidden size\n",
    "    768)\\\n",
    "-   **BERT-Large:** 340M parameters (24 layers, 16 heads, hidden size\n",
    "    1024)\n",
    "\n",
    "Where do these parameters come from?\n",
    "\n",
    "1.  **Embedding Layer:**\n",
    "    -   Vocab size â‰ˆ 30,000\\\n",
    "    -   Hidden size = 768 (base) or 1024 (large)\\\n",
    "    -   Params â‰ˆ 23M (base), 30M (large)\n",
    "2.  **Transformer Encoder Blocks:**\n",
    "    -   Each block = Multi-Head Attention + FFN\\\n",
    "    -   For base: \\~7M per block Ã— 12 = \\~85M\\\n",
    "    -   For large: \\~13M per block Ã— 24 = \\~310M\n",
    "3.  **Output Head:**\n",
    "    -   Small compared to the rest.\n",
    "\n",
    "**Grand Total** = 110M (base) or 340M (large)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE7gKkfYid7v"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqqEbh-dh0Hm"
   },
   "source": [
    "ðŸ‘‰ **Key Insight:**\\\n",
    "- **GPT = Autoregressive (predict next word, one direction)**\\\n",
    "- **BERT = Autoencoding (fill in blanks, bidirectional)**\n",
    "\n",
    "This is why GPT shines at writing stories, while BERT excels at\n",
    "understanding text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNkneWeXZKSUwo4WEgyHRd2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
